experiment_name: td3_train_real_async
seed: 42 

serial: # CHECK
  port: COM15 
  baudrate: 115200
  timeout: 0.1

env:
  setpoint: 500.0 # CHECK

  bounds:
    action:
      low: [0.0, 0.0, 0.0]
      high: [17.5, 55.0, 0.01]
    observation:
      low: [0.0, 0.0, 0.0]
      high: [10230.0, 4095.0, 10230.0]
    reward:
      low: [-10230.0]
      high: [0]

  mass: 1.0 # TODO: Убрать после решения проблемы с двойным созданием подключения
  k_linear: 1.0
  k_nonlinear: 1.0
  k_damping: 0.1

  process_noise_std: 0.05
  measurement_noise_std: 0.02

hydra:
  run:
    dir: experiments/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}

data:
  frames_per_batch: 50
  total_frames: 1_000_000 # Обучение идет приблизительно 30 мин

  buffer_size: 100_000

agent:
  use_lstm_policy: True
  python_based_lstm_policy: False

  lstm_hidden_size: 32
  lstm_num_layers: 1

  mlp_depth: 0
  mlp_num_cells: 32

  use_lstm_qvalue: True # TODO переименовать в q_
  python_based_lstm_qvalue: True

  q_mlp_depth: 0
  q_mlp_num_cells: 32

  q_lstm_hidden_size: 32
  q_lstm_num_layers: 1

  gamma: 0.99
  num_qvalue_nets: 2
  target_update_eps: 0.95

  learning_rate_actor: 1e-4
  learning_rate_critic: 1e-4

train:
  batch_size: 4
  
  update_to_data: 16
  update_actor_freq: 2 

  total_train_steps: 1_000_000_000 # Обучение идет до прерывания через CTRL + C