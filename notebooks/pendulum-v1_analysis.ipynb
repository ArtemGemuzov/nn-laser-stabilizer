{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ эксперимента pendulum-v1\n",
    "\n",
    "## Постановка эксперимента\n",
    "\n",
    "**Задача**: управление маятником (Gymnasium `Pendulum-v1`) с помощью нейросетевого контроллера, обученного методом TD3.\n",
    "\n",
    "**Окружение**: `Pendulum-v1`.\n",
    "- **Наблюдение**: `[cos(θ), sin(θ), θ̇]` — угол и угловая скорость маятника.\n",
    "- **Действие**: крутящий момент (torque) `∈ [-2, 2]`.\n",
    "- **Награда**: `−(θ² + 0.1·θ̇² + 0.001·torque²)` — штраф за отклонение от вертикали и за расход энергии.\n",
    "- **Эпизод**: 200 шагов.\n",
    "\n",
    "**Алгоритм**: TD3, MLP 256×256, `gamma = 0.98`, `tau = 0.005`, `policy_freq = 2`, `lr = 0.001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from nn_laser_stabilizer.config.config import load_config\n",
    "from nn_laser_stabilizer.paths import get_experiment_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"pendulum-v1\"\n",
    "EXPERIMENT_DATE = \"2026-02-14\"\n",
    "EXPERIMENT_TIME = \"13-38-10\"\n",
    "\n",
    "EXPERIMENT_DIR_PATH = get_experiment_dir(\n",
    "    experiment_name=EXPERIMENT_NAME, \n",
    "    experiment_date=EXPERIMENT_DATE, \n",
    "    experiment_time=EXPERIMENT_TIME)\n",
    "\n",
    "config = load_config(EXPERIMENT_DIR_PATH / \"config.yaml\")\n",
    "print(f\"Эксперимент: {config.experiment_name}\")\n",
    "print(f\"Окружение: {config.env.name}\")\n",
    "print(f\"Алгоритм: {config.algorithm.type}\")\n",
    "print(f\"Exploration steps: {config.exploration.steps}\")\n",
    "print(f\"Train start step: {config.training.train_start_step}\")\n",
    "print(f\"Evaluation frequency: {config.evaluation.frequency}\")\n",
    "print(f\"Log frequency: {config.training.log_frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка и парсинг данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path: Path, source: str | None = None, event: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Загрузка JSONL-файла с опциональной фильтрацией по source и event.\"\"\"\n",
    "    rows = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            if source and record.get(\"source\") != source:\n",
    "                continue\n",
    "            if event and record.get(\"event\") != event:\n",
    "                continue\n",
    "            rows.append(record)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LOG_PATH = EXPERIMENT_DIR_PATH / config.training.log_dir / config.training.log_file\n",
    "\n",
    "# Шаги обучения\n",
    "train_df = load_jsonl(TRAIN_LOG_PATH, source=\"train\", event=\"step\")\n",
    "print(f\"Шаги обучения: {len(train_df)} записей\")\n",
    "print(f\"Диапазон шагов: {train_df['step'].min()} — {train_df['step'].max()}\")\n",
    "\n",
    "# Evaluation\n",
    "eval_df = load_jsonl(TRAIN_LOG_PATH, source=\"train\", event=\"evaluation\")\n",
    "print(f\"\\nEvaluation: {len(eval_df)} записей\")\n",
    "if len(eval_df) > 0:\n",
    "    print(f\"Диапазон шагов: {eval_df['step'].min()} — {eval_df['step'].max()}\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: динамика награды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(eval_df['step'], eval_df['reward_mean'], 'o-', markersize=3, linewidth=1.0, color='tab:blue', label='reward_mean')\n",
    "plt.fill_between(eval_df['step'], eval_df['reward_min'], eval_df['reward_max'], alpha=0.2, color='tab:blue', label='reward min–max')\n",
    "plt.title('Evaluation: средняя награда за шаг')\n",
    "plt.xlabel('Шаг обучения')\n",
    "plt.ylabel('Reward (mean per step)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(eval_df['step'], eval_df['reward_sum'], 'o-', markersize=3, linewidth=1.0, color='tab:green', label='reward_sum')\n",
    "plt.title('Evaluation: суммарная награда за эпизод')\n",
    "plt.xlabel('Шаг обучения')\n",
    "plt.ylabel('Reward (sum)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ процесса обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(train_df['step'], train_df['loss_q1'], alpha=0.7, linewidth=0.8, color='tab:blue', label='Q1 Loss')\n",
    "plt.title('Critic Q1 Loss')\n",
    "plt.xlabel('Шаг обучения')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(train_df['step'], train_df['loss_q2'], alpha=0.7, linewidth=0.8, color='tab:green', label='Q2 Loss')\n",
    "plt.title('Critic Q2 Loss')\n",
    "plt.xlabel('Шаг обучения')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_df = train_df[train_df['actor_loss'].notna()]\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(actor_df['step'], actor_df['actor_loss'], alpha=0.7, linewidth=0.8, color='tab:red', label='Actor Loss')\n",
    "plt.title('Actor Loss')\n",
    "plt.xlabel('Шаг обучения')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(train_df['step'], train_df['buffer_size'], linewidth=1.0, color='tab:purple')\n",
    "plt.title('Размер буфера воспроизведения')\n",
    "plt.xlabel('Шаг обучения')\n",
    "plt.ylabel('Размер буфера')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_laser_stabilizer.rl.model.actor import MLPActor\n",
    "from nn_laser_stabilizer.rl.model.critic import MLPCritic\n",
    "\n",
    "MODELS_DIR = EXPERIMENT_DIR_PATH / \"models\"\n",
    "\n",
    "actor = MLPActor.load(MODELS_DIR / \"actor.pth\").eval()\n",
    "actor_target = MLPActor.load(MODELS_DIR / \"actor_target.pth\").eval()\n",
    "critic1 = MLPCritic.load(MODELS_DIR / \"critic1.pth\").eval()\n",
    "critic2 = MLPCritic.load(MODELS_DIR / \"critic2.pth\").eval()\n",
    "\n",
    "print(\"Actor:\")\n",
    "print(actor)\n",
    "print(f\"\\nВсего параметров actor: {sum(p.numel() for p in actor.parameters()):,}\")\n",
    "print(f\"Всего параметров critic1: {sum(p.numel() for p in critic1.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_histograms(model: torch.nn.Module, title: str):\n",
    "    \"\"\"Гистограммы весов и bias для каждого линейного слоя.\"\"\"\n",
    "    linear_layers = [(name, module) for name, module in model.named_modules()\n",
    "                     if isinstance(module, torch.nn.Linear)]\n",
    "    n = len(linear_layers)\n",
    "    fig, axes = plt.subplots(n, 2, figsize=(14, 3 * n))\n",
    "    if n == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, (name, layer) in enumerate(linear_layers):\n",
    "        w = layer.weight.detach().cpu().numpy().flatten()\n",
    "        axes[i, 0].hist(w, bins=80, alpha=0.7, color='tab:blue', edgecolor='black', linewidth=0.3)\n",
    "        axes[i, 0].set_title(f'{name} weights [{layer.weight.shape[1]}→{layer.weight.shape[0]}]')\n",
    "        axes[i, 0].set_ylabel('Частота')\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        b = layer.bias.detach().cpu().numpy().flatten()\n",
    "        axes[i, 1].hist(b, bins=40, alpha=0.7, color='tab:orange', edgecolor='black', linewidth=0.3)\n",
    "        axes[i, 1].set_title(f'{name} bias [{layer.bias.shape[0]}]')\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weight_histograms(actor, 'Распределение весов: Actor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weight_histograms(critic1, 'Распределение весов: Critic 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация политики\n",
    "\n",
    "Observation Pendulum-v1: `[cos(θ), sin(θ), θ̇]`.\n",
    "\n",
    "Heatmap: `action(cos(θ), sin(θ))` при `θ̇ = 0` — какой момент прикладывает агент в зависимости от угла маятника (при нулевой скорости)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Политика как функция угла θ при θ̇ = 0\n",
    "theta_range = np.linspace(-np.pi, np.pi, 500)\n",
    "obs_1d = np.stack([np.cos(theta_range), np.sin(theta_range), np.zeros_like(theta_range)], axis=1)\n",
    "obs_t = torch.tensor(obs_1d, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    actions_1d, _ = actor(obs_t)\n",
    "    actions_1d_np = actions_1d.cpu().numpy().flatten()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.degrees(theta_range), actions_1d_np, linewidth=2, color='tab:red')\n",
    "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "plt.axvline(x=0, color='green', linestyle='--', linewidth=1, alpha=0.7, label='θ=0 (вертикаль)')\n",
    "plt.xlabel('θ (градусы)')\n",
    "plt.ylabel('action (torque)')\n",
    "plt.title('Политика актора: torque(θ) при θ̇=0')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: action(θ, θ̇)\n",
    "grid_n = 200\n",
    "theta_grid_range = np.linspace(-np.pi, np.pi, grid_n)\n",
    "thetadot_range = np.linspace(-8, 8, grid_n)\n",
    "theta_grid, thetadot_grid = np.meshgrid(theta_grid_range, thetadot_range)\n",
    "\n",
    "obs_grid = np.stack([\n",
    "    np.cos(theta_grid.flatten()),\n",
    "    np.sin(theta_grid.flatten()),\n",
    "    thetadot_grid.flatten(),\n",
    "], axis=1)\n",
    "\n",
    "obs_tensor = torch.tensor(obs_grid, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    actions_grid, _ = actor(obs_tensor)\n",
    "    actions_np = actions_grid.cpu().numpy().reshape(grid_n, grid_n)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "im = plt.imshow(actions_np, extent=[-180, 180, -8, 8], origin='lower', aspect='auto', cmap='RdBu_r')\n",
    "plt.colorbar(im, label='action (torque)')\n",
    "plt.xlabel('θ (градусы)')\n",
    "plt.ylabel('θ̇ (рад/с)')\n",
    "plt.title('Политика актора: torque(θ, θ̇)')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение actor и actor_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2-расстояние между параметрами actor и actor_target\n",
    "l2_distances = {}\n",
    "for (name, p), (_, p_target) in zip(actor.named_parameters(), actor_target.named_parameters()):\n",
    "    l2 = (p - p_target).norm().item()\n",
    "    l2_distances[name] = l2\n",
    "\n",
    "print(\"L2-расстояние между actor и actor_target по слоям:\")\n",
    "for name, dist in l2_distances.items():\n",
    "    print(f\"  {name}: {dist:.6f}\")\n",
    "\n",
    "total_l2 = sum(\n",
    "    (p - p_t).pow(2).sum().item()\n",
    "    for p, p_t in zip(actor.parameters(), actor_target.parameters())\n",
    ") ** 0.5\n",
    "print(f\"\\nОбщее L2-расстояние: {total_l2:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разница действий actor vs actor_target на сетке θ × θ̇\n",
    "with torch.no_grad():\n",
    "    actions_target_grid, _ = actor_target(obs_tensor)\n",
    "    actions_target_np = actions_target_grid.cpu().numpy().reshape(grid_n, grid_n)\n",
    "\n",
    "diff_np = actions_np - actions_target_np\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "vmax = max(abs(diff_np.min()), abs(diff_np.max()))\n",
    "im = plt.imshow(diff_np, extent=[-180, 180, -8, 8], origin='lower', aspect='auto',\n",
    "                cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "plt.colorbar(im, label='action − action_target')\n",
    "plt.xlabel('θ (градусы)')\n",
    "plt.ylabel('θ̇ (рад/с)')\n",
    "plt.title('Разница политик: actor − actor_target')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ буфера воспроизведения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_laser_stabilizer.rl.data.replay_buffer import ReplayBuffer\n",
    "\n",
    "buffer = ReplayBuffer.load(EXPERIMENT_DIR_PATH / \"data\" / \"replay_buffer.pth\")\n",
    "n = buffer.size\n",
    "print(f\"Размер буфера: {n}\")\n",
    "print(f\"Ёмкость: {buffer.capacity}\")\n",
    "print(f\"obs_dim: {buffer.observations.shape[1]}, action_dim: {buffer.actions.shape[1]}\")\n",
    "\n",
    "buf_obs = buffer.observations[:n].numpy()       # (N, 3): cos(θ), sin(θ), θ̇\n",
    "buf_actions = buffer.actions[:n].numpy()         # (N, 1): torque\n",
    "buf_rewards = buffer.rewards[:n].numpy()         # (N, 1)\n",
    "buf_next_obs = buffer.next_observations[:n].numpy()\n",
    "buf_dones = buffer.dones[:n].numpy()\n",
    "\n",
    "print(f\"\\nСтатистика наблюдений:\")\n",
    "for i, name in enumerate(['cos(θ)', 'sin(θ)', 'θ̇']):\n",
    "    print(f\"  {name}: mean={buf_obs[:, i].mean():.4f}, std={buf_obs[:, i].std():.4f}, \"\n",
    "          f\"min={buf_obs[:, i].min():.4f}, max={buf_obs[:, i].max():.4f}\")\n",
    "\n",
    "print(f\"\\nСтатистика действий:\")\n",
    "print(f\"  torque: mean={buf_actions.mean():.4f}, std={buf_actions.std():.4f}, \"\n",
    "      f\"min={buf_actions.min():.4f}, max={buf_actions.max():.4f}\")\n",
    "\n",
    "print(f\"\\nСтатистика наград:\")\n",
    "print(f\"  reward: mean={buf_rewards.mean():.4f}, std={buf_rewards.std():.4f}, \"\n",
    "      f\"min={buf_rewards.min():.4f}, max={buf_rewards.max():.4f}\")\n",
    "\n",
    "print(f\"\\nDones: {buf_dones.sum():.0f} / {n} ({buf_dones.mean() * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].hist(buf_obs[:, 0], bins=100, alpha=0.7, color='tab:blue', edgecolor='black', linewidth=0.3)\n",
    "axes[0, 0].set_title('cos(θ)')\n",
    "axes[0, 0].set_ylabel('Частота')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(buf_obs[:, 1], bins=100, alpha=0.7, color='tab:cyan', edgecolor='black', linewidth=0.3)\n",
    "axes[0, 1].set_title('sin(θ)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].hist(buf_actions.flatten(), bins=100, alpha=0.7, color='tab:purple', edgecolor='black', linewidth=0.3)\n",
    "axes[1, 0].set_title('action (torque)')\n",
    "axes[1, 0].set_xlabel('Значение')\n",
    "axes[1, 0].set_ylabel('Частота')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].hist(buf_rewards.flatten(), bins=100, alpha=0.7, color='tab:green', edgecolor='black', linewidth=0.3)\n",
    "axes[1, 1].set_title('reward')\n",
    "axes[1, 1].set_xlabel('Значение')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Распределения данных в буфере воспроизведения', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Восстанавливаем θ из cos(θ) и sin(θ)\n",
    "theta_buf = np.arctan2(buf_obs[:, 1], buf_obs[:, 0])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(np.degrees(theta_buf), buf_obs[:, 2], c=buf_actions.flatten(), \n",
    "            cmap='RdBu_r', alpha=0.05, s=1)\n",
    "plt.colorbar(label='action (torque)')\n",
    "plt.xlabel('θ (градусы)')\n",
    "plt.ylabel('θ̇ (рад/с)')\n",
    "plt.title('Буфер: фазовый портрет, цвет = action')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.hist2d(np.degrees(theta_buf), buf_obs[:, 2], bins=100, cmap='hot_r')\n",
    "plt.colorbar(label='Количество переходов')\n",
    "plt.xlabel('θ (градусы)')\n",
    "plt.ylabel('θ̇ (рад/с)')\n",
    "plt.title('Покрытие пространства состояний в буфере')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка Q-функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_t = torch.tensor(buf_obs, dtype=torch.float32)\n",
    "act_t = torch.tensor(buf_actions, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    q1_buffer, _ = critic1(obs_t, act_t)\n",
    "    q2_buffer, _ = critic2(obs_t, act_t)\n",
    "    q_min_buffer = torch.min(q1_buffer, q2_buffer).numpy().flatten()\n",
    "    \n",
    "    actor_actions, _ = actor(obs_t)\n",
    "    q1_policy, _ = critic1(obs_t, actor_actions)\n",
    "    q2_policy, _ = critic2(obs_t, actor_actions)\n",
    "    q_min_policy = torch.min(q1_policy, q2_policy).numpy().flatten()\n",
    "\n",
    "print(f\"Q(s, a_buffer): mean={q_min_buffer.mean():.4f}, std={q_min_buffer.std():.4f}, \"\n",
    "      f\"min={q_min_buffer.min():.4f}, max={q_min_buffer.max():.4f}\")\n",
    "print(f\"Q(s, π(s)):     mean={q_min_policy.mean():.4f}, std={q_min_policy.std():.4f}, \"\n",
    "      f\"min={q_min_policy.min():.4f}, max={q_min_policy.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(q_min_buffer, bins=100, alpha=0.7, color='tab:blue', edgecolor='black', linewidth=0.3)\n",
    "axes[0].set_title('Q(s, a) — действия из буфера')\n",
    "axes[0].set_xlabel('Q-value')\n",
    "axes[0].set_ylabel('Частота')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(q_min_policy, bins=100, alpha=0.7, color='tab:red', edgecolor='black', linewidth=0.3)\n",
    "axes[1].set_title('Q(s, π(s)) — действия актора')\n",
    "axes[1].set_xlabel('Q-value')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Распределение Q-значений (min(Q1, Q2))', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap Q(s, π(s)) на сетке θ × θ̇\n",
    "with torch.no_grad():\n",
    "    grid_actions, _ = actor(obs_tensor)\n",
    "    q1_grid, _ = critic1(obs_tensor, grid_actions)\n",
    "    q2_grid, _ = critic2(obs_tensor, grid_actions)\n",
    "    q_grid = torch.min(q1_grid, q2_grid).numpy().reshape(grid_n, grid_n)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "im = plt.imshow(q_grid, extent=[-180, 180, -8, 8], origin='lower', aspect='auto', cmap='viridis')\n",
    "plt.colorbar(im, label='Q(s, π(s))')\n",
    "plt.xlabel('θ (градусы)')\n",
    "plt.ylabel('θ̇ (рад/с)')\n",
    "plt.title('Q-значение политики: Q(s, π(s))')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn-laser-stabilizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
