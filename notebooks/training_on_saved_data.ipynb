{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51985726",
   "metadata": {},
   "source": [
    "# Эксперимент по обучению с использованием сохраненных данных из буфера воспроизведения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from torchrl.data import LazyTensorStorage, TensorDictReplayBuffer\n",
    "\n",
    "def make_buffer(config):\n",
    "    buffer = TensorDictReplayBuffer(\n",
    "        batch_size=1,\n",
    "        storage=LazyTensorStorage(max_size=config.data.buffer_size),\n",
    "    )\n",
    "    return buffer\n",
    "\n",
    "EXPERIMENT_NAME = \"td3_train_real_async\"\n",
    "EXPERIMENT_DATE = \"2025-09-30\"\n",
    "EXPERIMENT_TIME = \"13-08-38\"\n",
    "\n",
    "PATH_TO_EXP_DIR = Path(f\"../experiments/{EXPERIMENT_NAME}/{EXPERIMENT_DATE}_{EXPERIMENT_TIME}\")\n",
    "\n",
    "ENV_LOG_DIR = PATH_TO_EXP_DIR / \"env_logs\"\n",
    "TRAIN_LOG_DIR = PATH_TO_EXP_DIR / \"train_logs\"\n",
    "\n",
    "config_path = PATH_TO_EXP_DIR / \".hydra\" / \"config.yaml\"\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "\n",
    "buffer = make_buffer(config)\n",
    "\n",
    "buffer_path = PATH_TO_EXP_DIR / \"saved_data/replay_buffer.pkl\"\n",
    "buffer.load(buffer_path) \n",
    "\n",
    "print(f\"Replay buffer загружен. Размер: {len(buffer)} сэмплов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "batch = buffer.sample(batch_size)\n",
    "print(f\"Сэмплирован batch из {len(batch)} сэмплов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cfe1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = batch[0]\n",
    "\n",
    "print(\"Поля сэмпла:\", sample.keys())\n",
    "\n",
    "print(\"\\nObservation:\", sample[\"observation\"])\n",
    "print(\"Next Observation:\", sample[\"next\"][\"observation\"])\n",
    "print(\"Action:\", sample[\"action\"])\n",
    "print(\"Reward:\", sample[\"next\"][\"reward\"])\n",
    "print(\"Done:\", sample[\"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_buffer = make_buffer(config)\n",
    "buffer._batch_size = 1\n",
    "seen = set()\n",
    "\n",
    "for sample in buffer:\n",
    "    action = sample[\"action\"]\n",
    "    \n",
    "    if (action[0] > 0.9).any() or (action[0] < -0.9).any():\n",
    "        continue\n",
    "\n",
    "    obs_tuple = tuple(sample[\"observation\"].flatten().tolist())\n",
    "    action_tuple = tuple(action.flatten().tolist())\n",
    "    key = (obs_tuple, action_tuple)\n",
    "    \n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "\n",
    "    observation = sample[\"observation\"]\n",
    "\n",
    "    observation[0][0] = observation[0][0] / 125.0\n",
    "    observation[0][1] = observation[0][1] / 10.0  \n",
    "    sample[\"observation\"] = observation\n",
    "\n",
    "    next_observation = sample[\"next\"][\"observation\"].clone()\n",
    "    next_observation[0][0] = next_observation[0][0] / 125.0\n",
    "    next_observation[0][1] = next_observation[0][1] / 10.0\n",
    "    sample[\"next\"][\"observation\"] = next_observation\n",
    "    \n",
    "    filtered_buffer.add(sample)\n",
    "\n",
    "    if (len(filtered_buffer) > 2500):\n",
    "        break\n",
    "\n",
    "print(f\"Filtered buffer создан. Размер: {len(filtered_buffer)} сэмплов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61203402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from nn_laser_stabilizer.envs.utils import make_specs\n",
    "from nn_laser_stabilizer.logging.utils import set_seeds\n",
    "from nn_laser_stabilizer.agents.td3 import (\n",
    "    make_td3_agent,\n",
    "    make_loss_module,\n",
    "    make_optimizers,\n",
    "    make_target_updater,\n",
    "    train_step,\n",
    "    warmup_from_specs\n",
    ")\n",
    "\n",
    "set_seeds(config.seed)\n",
    "\n",
    "specs = make_specs(config.env.bounds)\n",
    "action_spec = specs[\"action\"]\n",
    "observation_spec = specs[\"observation\"]\n",
    "\n",
    "config.agent.learning_rate_actor = 1e-4\n",
    "\n",
    "actor, qvalue = make_td3_agent(config, observation_spec, action_spec)\n",
    "warmup_from_specs(observation_spec, action_spec, actor, qvalue)\n",
    "\n",
    "loss_module = make_loss_module(config, actor, qvalue, action_spec)\n",
    "optimizer_actor, optimizer_critic = make_optimizers(config, loss_module)\n",
    "target_net_updater = make_target_updater(config, loss_module)\n",
    "\n",
    "train_config = config.train\n",
    "\n",
    "total_train_steps = 0\n",
    "recent_qvalue_losses = deque(maxlen=train_config.update_to_data)\n",
    "recent_actor_losses = deque(maxlen=train_config.update_to_data // train_config.update_actor_freq)\n",
    "\n",
    "print(\"Training process initiated\")\n",
    "\n",
    "try:\n",
    "    for _ in range(100):\n",
    "        try:\n",
    "            for i in range(train_config.update_to_data):\n",
    "                batch = filtered_buffer.sample(train_config.batch_size)\n",
    "                update_actor = i % train_config.update_actor_freq == 0\n",
    "                loss_qvalue_val, loss_actor_val = train_step(\n",
    "                    batch, loss_module, optimizer_actor, optimizer_critic,\n",
    "                    target_net_updater, update_actor\n",
    "                )\n",
    "\n",
    "                recent_qvalue_losses.append(loss_qvalue_val)\n",
    "                if loss_actor_val is not None:\n",
    "                    recent_actor_losses.append(loss_actor_val)\n",
    "\n",
    "            avg_qvalue_loss = sum(recent_qvalue_losses) / len(recent_qvalue_losses)\n",
    "            avg_actor_loss = sum(recent_actor_losses) / len(recent_actor_losses)\n",
    "            print(f\"step={total_train_steps} Loss/Critic={avg_qvalue_loss} Loss/Actor={avg_actor_loss}\")\n",
    "\n",
    "            total_train_steps += 1\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Training interrupted by user.\")\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"Error while training: {ex}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Training finished\")\n",
    "    print(f\"Final buffer size: {len(filtered_buffer)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3222fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    sample = filtered_buffer.sample(1)[0]  \n",
    "    sample_action = sample[\"action\"]\n",
    "\n",
    "    actor.eval()\n",
    "    with torch.no_grad():\n",
    "        action = actor(sample)\n",
    "\n",
    "    print(\"Observation:\", sample[\"observation\"])\n",
    "    print(\"Action (from buffer):\", sample_action)\n",
    "    print(\"Action predicted by actor:\", action[\"action\"])\n",
    "\n",
    "    qvalue.eval()\n",
    "    with torch.no_grad():\n",
    "        q_pred = qvalue(sample)  \n",
    "\n",
    "    print(\"Predicted Q-value:\", q_pred[\"state_action_value\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58dfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mean_list = []\n",
    "error_std_list = []\n",
    "kp_list = []\n",
    "ki_list = []\n",
    "kd_list = []\n",
    "reward_list = []\n",
    "done_list = []\n",
    "\n",
    "for i in range(len(buffer)):\n",
    "    sample = buffer[i]\n",
    "    \n",
    "    error_mean_list.append(sample[\"observation\"][0].cpu())\n",
    "    error_std_list.append(sample[\"observation\"][1].cpu())\n",
    "    \n",
    "    kp_list.append(sample[\"action\"][0].cpu())\n",
    "    ki_list.append(sample[\"action\"][1].cpu())\n",
    "    kd_list.append(sample[\"action\"][2].cpu())\n",
    "    \n",
    "    reward_list.append(sample[\"next\"][\"reward\"].cpu())\n",
    "    done_list.append(sample[\"done\"].cpu())\n",
    "\n",
    "error_mean = torch.stack(error_mean_list)\n",
    "error_std = torch.stack(error_std_list)\n",
    "kp = torch.stack(kp_list)\n",
    "ki = torch.stack(ki_list)\n",
    "kd = torch.stack(kd_list)\n",
    "reward = torch.stack(reward_list)\n",
    "done = torch.stack(done_list)\n",
    "\n",
    "def stats(tensor, name):\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(\"Min:\", tensor.min().item())\n",
    "    print(\"Max:\", tensor.max().item())\n",
    "    print(\"Mean:\", tensor.mean().item())\n",
    "    print(\"Std:\", tensor.std().item())\n",
    "    print()\n",
    "\n",
    "stats(error_mean, \"Error Mean (obs[0])\")\n",
    "stats(error_std, \"Error Std (obs[1])\")\n",
    "stats(kp, \"KP\")\n",
    "stats(ki, \"KI\")\n",
    "stats(kd, \"KD\")\n",
    "stats(reward, \"Reward\")\n",
    "stats(done.float(), \"Done flags\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn-laser-stabilizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
