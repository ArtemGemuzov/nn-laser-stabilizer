{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Анализ эксперимента neural_controller-v1\n",
        "\n",
        "## Постановка эксперимента\n",
        "\n",
        "**Задача**: стабилизация лазера с помощью нейросетевого контроллера, обученного методом Twin Delayed DDPG (TD3).\n",
        "\n",
        "**Окружение**: `NeuralPIDDeltaEnv` — агент управляет фазовращателем через последовательный порт (COM3). \n",
        "- **Наблюдение**: `[error, error_prev, error_prev_prev]` — текущая и две предыдущие ошибки (нормализованы).\n",
        "- **Действие**: нормализованное приращение управляющего сигнала `delta_norm ∈ [-1, 1]`, масштабируемое до `delta ∈ [-30, 30]`.\n",
        "- **Награда**: `-|error|` — минимизация абсолютной ошибки.\n",
        "- **Уставка (setpoint)**: 1200 (относительно `process_variable_max = 10230`).\n",
        "- **Управляющий сигнал**: `control_output ∈ [0, 4095]`, начальное значение при ресете — 2000.\n",
        "\n",
        "**Алгоритм**: TD3, MLP 256×256, `gamma = 0.99`, `tau = 0.005`, `policy_freq = 2`.\n",
        "\n",
        "**Сбор данных**: асинхронный коллектор, синхронизация весов каждые 10 шагов.\n",
        "\n",
        "**Исследование (exploration)**: PID-контроллер (`kp=3.5, ki=11.0, kd=0.002`) в течение первых 20 000 шагов окружения.\n",
        "\n",
        "**Обучение**: начинается после 5 000 шагов буфера, буфер на 100 000 переходов, батч 256.\n",
        "\n",
        "**Длительность**: 2026-02-13 14:59 → 16:13 (~1 ч 14 мин), прерван пользователем."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from nn_laser_stabilizer.config.config import load_config\n",
        "from nn_laser_stabilizer.paths import get_experiment_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"neural_controller-v1\"\n",
        "EXPERIMENT_DATE = \"2026-02-13\"\n",
        "EXPERIMENT_TIME = \"14-59-23\"\n",
        "\n",
        "EXPERIMENT_DIR_PATH = get_experiment_dir(\n",
        "    experiment_name=EXPERIMENT_NAME, \n",
        "    experiment_date=EXPERIMENT_DATE, \n",
        "    experiment_time=EXPERIMENT_TIME)\n",
        "\n",
        "config = load_config(EXPERIMENT_DIR_PATH / \"config.yaml\")\n",
        "print(f\"Эксперимент: {config.experiment_name}\")\n",
        "print(f\"Окружение: {config.env.name}\")\n",
        "print(f\"Алгоритм: {config.algorithm.type}\")\n",
        "print(f\"Exploration steps: {config.exploration.steps}\")\n",
        "print(f\"Train start step: {config.training.train_start_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Загрузка и парсинг данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_jsonl(path: Path, source: str | None = None, event: str | None = None) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                record = json.loads(line)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "            if source and record.get(\"source\") != source:\n",
        "                continue\n",
        "            if event and record.get(\"event\") != event:\n",
        "                continue\n",
        "            rows.append(record)\n",
        "    return pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_df = load_jsonl(EXPERIMENT_DIR_PATH / \"env.jsonl\", source=\"env\", event=\"step\")\n",
        "env_df = env_df.reset_index(drop=True)\n",
        "env_df['global_step'] = env_df.index + 1  # абсолютный номер шага (step сбрасывается при reset)\n",
        "print(f\"Шаги окружения: {len(env_df)} записей\")\n",
        "print(f\"Диапазон global_step: {env_df['global_step'].min()} — {env_df['global_step'].max()}\")\n",
        "env_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = load_jsonl(EXPERIMENT_DIR_PATH / \"train.jsonl\", source=\"train\", event=\"step\")\n",
        "print(f\"Шаги обучения: {len(train_df)} записей\")\n",
        "print(f\"Диапазон шагов: {train_df['step'].min()} — {train_df['step'].max()}\")\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exploration_steps = config.exploration.steps\n",
        "print(f\"Exploration steps: {exploration_steps}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Анализ логов окружения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(env_df['global_step'], env_df['error'], alpha=0.8, linewidth=0.5, label='Error')\n",
        "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)\n",
        "plt.axvline(x=exploration_steps, color='red', linestyle='--', linewidth=2,\n",
        "            label=f'Переход на НС (шаг {exploration_steps})')\n",
        "plt.title('Ошибка регулирования')\n",
        "plt.xlabel('Шаг окружения (абсолютный)')\n",
        "plt.ylabel('Error')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(env_df['global_step'], env_df['reward'], alpha=0.8, linewidth=0.5, color='tab:green', label='Reward')\n",
        "plt.axvline(x=exploration_steps, color='red', linestyle='--', linewidth=2,\n",
        "            label=f'Переход на НС (шаг {exploration_steps})')\n",
        "plt.title('Награда')\n",
        "plt.xlabel('Шаг окружения (абсолютный)')\n",
        "plt.ylabel('Reward')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(env_df['global_step'], env_df['process_variable'], 'b-', alpha=0.7, linewidth=0.5, label='Process Variable')\n",
        "plt.plot(env_df['global_step'], env_df['setpoint'], 'r--', linewidth=1.5, label='Setpoint')\n",
        "plt.axvline(x=exploration_steps, color='orange', linestyle='--', linewidth=2,\n",
        "            label=f'Переход на НС (шаг {exploration_steps})')\n",
        "plt.xlabel('Шаг окружения (абсолютный)')\n",
        "plt.ylabel('Значение')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(env_df['global_step'], env_df['control_output'], 'g-', alpha=0.7, linewidth=0.5, label='Control Output')\n",
        "plt.axvline(x=exploration_steps, color='red', linestyle='--', linewidth=2,\n",
        "            label=f'Переход на НС (шаг {exploration_steps})')\n",
        "plt.title('Управляющий сигнал')\n",
        "plt.xlabel('Шаг окружения (абсолютный)')\n",
        "plt.ylabel('Control Output')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(env_df['global_step'], env_df['delta_norm'], alpha=0.7, linewidth=0.5, color='tab:purple', label='Delta (norm)')\n",
        "plt.axvline(x=exploration_steps, color='red', linestyle='--', linewidth=2,\n",
        "            label=f'Переход на НС (шаг {exploration_steps})')\n",
        "plt.title('Действие агента (нормализованное приращение)')\n",
        "plt.xlabel('Шаг окружения (абсолютный)')\n",
        "plt.ylabel('delta_norm')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(env_df['global_step'], env_df['step_interval_us'], alpha=0.7, linewidth=0.5, color='tab:brown')\n",
        "plt.axvline(x=exploration_steps, color='red', linestyle='--', linewidth=2,\n",
        "            label=f'Переход на НС (шаг {exploration_steps})')\n",
        "plt.title('Интервал между шагами')\n",
        "plt.xlabel('Шаг окружения (абсолютный)')\n",
        "plt.ylabel('Интервал (мкс)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Сравнение PID-exploration vs НС-контроллер"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pid_df = env_df[env_df['global_step'] <= exploration_steps]\n",
        "nn_df = env_df[env_df['global_step'] > exploration_steps]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].hist(pid_df['error'], bins=100, alpha=0.7, color='tab:blue', edgecolor='black', linewidth=0.3)\n",
        "axes[0].set_title(f'Распределение ошибки: PID (шаги 1–{exploration_steps})')\n",
        "axes[0].set_xlabel('Error')\n",
        "axes[0].set_ylabel('Частота')\n",
        "axes[0].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].hist(nn_df['error'], bins=100, alpha=0.7, color='tab:orange', edgecolor='black', linewidth=0.3)\n",
        "axes[1].set_title(f'Распределение ошибки: НС (шаги {exploration_steps + 1}–{env_df[\"global_step\"].max()})')\n",
        "axes[1].set_xlabel('Error')\n",
        "axes[1].set_ylabel('Частота')\n",
        "axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"PID  — mean |error|: {pid_df['error'].abs().mean():.2f}, std: {pid_df['error'].std():.2f}, median: {pid_df['error'].median():.2f}\")\n",
        "print(f\"НС   — mean |error|: {nn_df['error'].abs().mean():.2f}, std: {nn_df['error'].std():.2f}, median: {nn_df['error'].median():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Скользящее среднее ошибки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "alpha = 0.2\n",
        "env_df['abs_error_ema'] = env_df['error'].abs().ewm(alpha=alpha, adjust=False).mean()\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(env_df['global_step'], env_df['abs_error_ema'], linewidth=1.0, color='tab:red',\n",
        "         label=f'|Error| (EMA, α={alpha})')\n",
        "plt.axvline(x=exploration_steps, color='gray', linestyle='--', linewidth=2,\n",
        "            label=f'Переход на НС (шаг {exploration_steps})')\n",
        "plt.title('Экспоненциальное скользящее среднее абсолютной ошибки')\n",
        "plt.xlabel('Шаг окружения (абсолютный)')\n",
        "plt.ylabel('|Error| (EMA)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Анализ процесса обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(train_df['step'], train_df['loss_q1'], alpha=0.6, linewidth=0.3, color='tab:blue', label='Q1 Loss')\n",
        "plt.title('Critic Q1 Loss')\n",
        "plt.xlabel('Шаг обучения')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(train_df['step'], train_df['loss_q2'], alpha=0.6, linewidth=0.3, color='tab:green', label='Q2 Loss')\n",
        "plt.title('Critic Q2 Loss')\n",
        "plt.xlabel('Шаг обучения')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "actor_df = train_df[train_df['actor_loss'].notna()]\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(actor_df['step'], actor_df['actor_loss'], alpha=0.6, linewidth=0.3, color='tab:red', label='Actor Loss')\n",
        "plt.title('Actor Loss')\n",
        "plt.xlabel('Шаг обучения')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(train_df['step'], train_df['buffer_size'], linewidth=1.0, color='tab:purple')\n",
        "plt.title('Размер буфера воспроизведения')\n",
        "plt.xlabel('Шаг обучения')\n",
        "plt.ylabel('Размер буфера')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Анализ логов соединения (phase_shifter)\n",
        "\n",
        "Логи `source=phase_shifter, event=exchange` — сырые обмены с фазовращателем через последовательный порт. Каждая запись содержит отправленный `control_output` и полученный `process_variable`. Эти данные включают все обмены, в том числе во время reset-фаз окружения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ps_df = load_jsonl(EXPERIMENT_DIR_PATH / \"env.jsonl\", source=\"phase_shifter\", event=\"exchange\")\n",
        "ps_df = ps_df.reset_index(drop=True)\n",
        "ps_df['global_step'] = ps_df.index + 1\n",
        "print(f\"Обмены с фазовращателем: {len(ps_df)} записей\")\n",
        "print(f\"control_output: [{ps_df['control_output'].min()}, {ps_df['control_output'].max()}]\")\n",
        "print(f\"process_variable: [{ps_df['process_variable'].min()}, {ps_df['process_variable'].max()}]\")\n",
        "ps_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(ps_df['global_step'], ps_df['process_variable'], alpha=0.7, linewidth=0.3, color='tab:blue')\n",
        "plt.title('Process Variable (логи соединения)')\n",
        "plt.xlabel('Номер обмена')\n",
        "plt.ylabel('Process Variable')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(ps_df['global_step'], ps_df['control_output'], alpha=0.7, linewidth=0.3, color='tab:green')\n",
        "plt.title('Control Output (логи соединения)')\n",
        "plt.xlabel('Номер обмена')\n",
        "plt.ylabel('Control Output')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
        "\n",
        "axes[0].plot(ps_df['global_step'], ps_df['control_output'], alpha=0.7, linewidth=0.3, color='tab:green')\n",
        "axes[0].set_title('Control Output')\n",
        "axes[0].set_ylabel('Control Output')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(ps_df['global_step'], ps_df['process_variable'], alpha=0.7, linewidth=0.3, color='tab:blue')\n",
        "axes[1].set_title('Process Variable')\n",
        "axes[1].set_xlabel('Номер обмена')\n",
        "axes[1].set_ylabel('Process Variable')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Логи соединения: Control Output и Process Variable', fontsize=13)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Анализ моделей\n",
        "\n",
        "Загрузка сохранённых моделей: actor, critic1, critic2 и их target-копии."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from nn_laser_stabilizer.rl.model.actor import MLPActor\n",
        "from nn_laser_stabilizer.rl.model.critic import MLPCritic\n",
        "\n",
        "MODELS_DIR = EXPERIMENT_DIR_PATH / \"models\"\n",
        "\n",
        "actor = MLPActor.load(MODELS_DIR / \"actor.pth\").eval()\n",
        "actor_target = MLPActor.load(MODELS_DIR / \"actor_target.pth\").eval()\n",
        "critic1 = MLPCritic.load(MODELS_DIR / \"critic1.pth\").eval()\n",
        "critic2 = MLPCritic.load(MODELS_DIR / \"critic2.pth\").eval()\n",
        "critic1_target = MLPCritic.load(MODELS_DIR / \"critic1_target.pth\").eval()\n",
        "critic2_target = MLPCritic.load(MODELS_DIR / \"critic2_target.pth\").eval()\n",
        "\n",
        "print(\"Actor:\")\n",
        "print(actor)\n",
        "print(f\"\\nВсего параметров actor: {sum(p.numel() for p in actor.parameters()):,}\")\n",
        "print(f\"Всего параметров critic1: {sum(p.numel() for p in critic1.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_weight_histograms(model: torch.nn.Module, title: str):\n",
        "    \"\"\"Гистограммы весов и bias для каждого линейного слоя.\"\"\"\n",
        "    linear_layers = [(name, module) for name, module in model.named_modules()\n",
        "                     if isinstance(module, torch.nn.Linear)]\n",
        "    n = len(linear_layers)\n",
        "    fig, axes = plt.subplots(n, 2, figsize=(14, 3 * n))\n",
        "    if n == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    for i, (name, layer) in enumerate(linear_layers):\n",
        "        w = layer.weight.detach().cpu().numpy().flatten()\n",
        "        axes[i, 0].hist(w, bins=80, alpha=0.7, color='tab:blue', edgecolor='black', linewidth=0.3)\n",
        "        axes[i, 0].set_title(f'{name} weights [{layer.weight.shape[1]}→{layer.weight.shape[0]}]')\n",
        "        axes[i, 0].set_ylabel('Частота')\n",
        "        axes[i, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        b = layer.bias.detach().cpu().numpy().flatten()\n",
        "        axes[i, 1].hist(b, bins=40, alpha=0.7, color='tab:orange', edgecolor='black', linewidth=0.3)\n",
        "        axes[i, 1].set_title(f'{name} bias [{layer.bias.shape[0]}]')\n",
        "        axes[i, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_weight_histograms(actor, 'Распределение весов: Actor')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_weight_histograms(critic1, 'Распределение весов: Critic 1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Визуализация политики: heatmap action(error, error_prev)\n",
        "\n",
        "Подаём на вход актору сетку значений `(error, error_prev)` при фиксированном `error_prev_prev = 0` и строим heatmap выходного действия. Значения ошибок нормализованы в `[-1, 1]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_n = 200\n",
        "error_range = np.linspace(-1, 1, grid_n)\n",
        "error_prev_range = np.linspace(-1, 1, grid_n)\n",
        "error_grid, error_prev_grid = np.meshgrid(error_range, error_prev_range)\n",
        "\n",
        "# error_prev_prev = 0 (фиксируем)\n",
        "obs_grid = np.stack([\n",
        "    error_grid.flatten(),\n",
        "    error_prev_grid.flatten(),\n",
        "    np.zeros(grid_n * grid_n),\n",
        "], axis=1)\n",
        "\n",
        "obs_tensor = torch.tensor(obs_grid, dtype=torch.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    actions, _ = actor(obs_tensor)\n",
        "    actions_np = actions.cpu().numpy().reshape(grid_n, grid_n)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "im = plt.imshow(actions_np, extent=[-1, 1, -1, 1], origin='lower', aspect='auto', cmap='RdBu_r')\n",
        "plt.colorbar(im, label='action (delta_norm)')\n",
        "plt.xlabel('error (norm)')\n",
        "plt.ylabel('error_prev (norm)')\n",
        "plt.title('Политика актора: action(error, error_prev) при error_prev_prev=0')\n",
        "plt.grid(False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Срез: action(error) при error_prev=0, error_prev_prev=0\n",
        "obs_1d = torch.tensor(\n",
        "    np.stack([error_range, np.zeros(grid_n), np.zeros(grid_n)], axis=1),\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    actions_1d, _ = actor(obs_1d)\n",
        "    actions_1d_np = actions_1d.cpu().numpy().flatten()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(error_range, actions_1d_np, linewidth=2, color='tab:red')\n",
        "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)\n",
        "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)\n",
        "plt.xlabel('error (norm)')\n",
        "plt.ylabel('action (delta_norm)')\n",
        "plt.title('Срез политики: action(error) при error_prev=0, error_prev_prev=0')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Сравнение actor и actor_target\n",
        "\n",
        "L2-расстояние между параметрами основной и целевой сети, а также разница в действиях на одних и тех же входах."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# L2-расстояние между параметрами actor и actor_target\n",
        "l2_distances = {}\n",
        "for (name, p), (_, p_target) in zip(actor.named_parameters(), actor_target.named_parameters()):\n",
        "    l2 = (p - p_target).norm().item()\n",
        "    l2_distances[name] = l2\n",
        "\n",
        "print(\"L2-расстояние между actor и actor_target по слоям:\")\n",
        "for name, dist in l2_distances.items():\n",
        "    print(f\"  {name}: {dist:.6f}\")\n",
        "\n",
        "total_l2 = sum(\n",
        "    (p - p_t).pow(2).sum().item()\n",
        "    for p, p_t in zip(actor.parameters(), actor_target.parameters())\n",
        ") ** 0.5\n",
        "print(f\"\\nОбщее L2-расстояние: {total_l2:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Разница действий actor vs actor_target на сетке\n",
        "with torch.no_grad():\n",
        "    actions_target, _ = actor_target(obs_tensor)\n",
        "    actions_target_np = actions_target.cpu().numpy().reshape(grid_n, grid_n)\n",
        "\n",
        "diff_np = actions_np - actions_target_np\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "vmax = max(abs(diff_np.min()), abs(diff_np.max()))\n",
        "im = plt.imshow(diff_np, extent=[-1, 1, -1, 1], origin='lower', aspect='auto',\n",
        "                cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
        "plt.colorbar(im, label='action - action_target')\n",
        "plt.xlabel('error (norm)')\n",
        "plt.ylabel('error_prev (norm)')\n",
        "plt.title('Разница политик: actor − actor_target (error_prev_prev=0)')\n",
        "plt.grid(False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Анализ буфера воспроизведения\n",
        "\n",
        "Загрузка сохранённого `ReplayBuffer` и анализ распределений наблюдений, действий и наград."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nn_laser_stabilizer.rl.data.replay_buffer import ReplayBuffer\n",
        "\n",
        "buffer = ReplayBuffer.load(EXPERIMENT_DIR_PATH / \"data\" / \"replay_buffer.pth\")\n",
        "n = buffer.size\n",
        "print(f\"Размер буфера: {n}\")\n",
        "print(f\"Ёмкость: {buffer.capacity}\")\n",
        "print(f\"obs_dim: {buffer.observations.shape[1]}, action_dim: {buffer.actions.shape[1]}\")\n",
        "\n",
        "buf_obs = buffer.observations[:n].numpy()       # (N, 3): error, error_prev, error_prev_prev\n",
        "buf_actions = buffer.actions[:n].numpy()         # (N, 1): delta_norm\n",
        "buf_rewards = buffer.rewards[:n].numpy()         # (N, 1)\n",
        "buf_next_obs = buffer.next_observations[:n].numpy()\n",
        "buf_dones = buffer.dones[:n].numpy()\n",
        "\n",
        "print(f\"\\nСтатистика наблюдений (нормализованные):\")\n",
        "for i, name in enumerate(['error', 'error_prev', 'error_prev_prev']):\n",
        "    print(f\"  {name}: mean={buf_obs[:, i].mean():.4f}, std={buf_obs[:, i].std():.4f}, \"\n",
        "          f\"min={buf_obs[:, i].min():.4f}, max={buf_obs[:, i].max():.4f}\")\n",
        "\n",
        "print(f\"\\nСтатистика действий:\")\n",
        "print(f\"  delta_norm: mean={buf_actions.mean():.4f}, std={buf_actions.std():.4f}, \"\n",
        "      f\"min={buf_actions.min():.4f}, max={buf_actions.max():.4f}\")\n",
        "\n",
        "print(f\"\\nСтатистика наград:\")\n",
        "print(f\"  reward: mean={buf_rewards.mean():.4f}, std={buf_rewards.std():.4f}, \"\n",
        "      f\"min={buf_rewards.min():.4f}, max={buf_rewards.max():.4f}\")\n",
        "\n",
        "print(f\"\\nDones: {buf_dones.sum()} / {n} ({buf_dones.mean() * 100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "axes[0, 0].hist(buf_obs[:, 0], bins=100, alpha=0.7, color='tab:blue', edgecolor='black', linewidth=0.3)\n",
        "axes[0, 0].set_title('error (norm)')\n",
        "axes[0, 0].set_ylabel('Частота')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].hist(buf_obs[:, 1], bins=100, alpha=0.7, color='tab:cyan', edgecolor='black', linewidth=0.3)\n",
        "axes[0, 1].set_title('error_prev (norm)')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 0].hist(buf_actions.flatten(), bins=100, alpha=0.7, color='tab:purple', edgecolor='black', linewidth=0.3)\n",
        "axes[1, 0].set_title('action (delta_norm)')\n",
        "axes[1, 0].set_xlabel('Значение')\n",
        "axes[1, 0].set_ylabel('Частота')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].hist(buf_rewards.flatten(), bins=100, alpha=0.7, color='tab:green', edgecolor='black', linewidth=0.3)\n",
        "axes[1, 1].set_title('reward')\n",
        "axes[1, 1].set_xlabel('Значение')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Распределения данных в буфере воспроизведения', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(buf_obs[:, 0], buf_actions.flatten(), alpha=0.05, s=1, color='tab:purple')\n",
        "plt.xlabel('error (norm)')\n",
        "plt.ylabel('action (delta_norm)')\n",
        "plt.title('Буфер: зависимость action от error')\n",
        "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)\n",
        "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.hist2d(buf_obs[:, 0], buf_obs[:, 1], bins=100, cmap='hot_r')\n",
        "plt.colorbar(label='Количество переходов')\n",
        "plt.xlabel('error (norm)')\n",
        "plt.ylabel('error_prev (norm)')\n",
        "plt.title('Покрытие пространства состояний в буфере')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Оценка Q-функции\n",
        "\n",
        "Оценка Q(s, a) на данных из буфера: как критик оценивает качество действий, накопленных в буфере. Также — Q-значение для действий, предложенных актором (Q(s, π(s)))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "obs_t = torch.tensor(buf_obs, dtype=torch.float32)\n",
        "act_t = torch.tensor(buf_actions, dtype=torch.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Q(s, a) — оценка действий из буфера\n",
        "    q1_buffer, _ = critic1(obs_t, act_t)\n",
        "    q2_buffer, _ = critic2(obs_t, act_t)\n",
        "    q_min_buffer = torch.min(q1_buffer, q2_buffer).numpy().flatten()\n",
        "    \n",
        "    # Q(s, π(s)) — оценка действий актора\n",
        "    actor_actions, _ = actor(obs_t)\n",
        "    q1_policy, _ = critic1(obs_t, actor_actions)\n",
        "    q2_policy, _ = critic2(obs_t, actor_actions)\n",
        "    q_min_policy = torch.min(q1_policy, q2_policy).numpy().flatten()\n",
        "\n",
        "print(f\"Q(s, a_buffer): mean={q_min_buffer.mean():.4f}, std={q_min_buffer.std():.4f}, \"\n",
        "      f\"min={q_min_buffer.min():.4f}, max={q_min_buffer.max():.4f}\")\n",
        "print(f\"Q(s, π(s)):     mean={q_min_policy.mean():.4f}, std={q_min_policy.std():.4f}, \"\n",
        "      f\"min={q_min_policy.min():.4f}, max={q_min_policy.max():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].hist(q_min_buffer, bins=100, alpha=0.7, color='tab:blue', edgecolor='black', linewidth=0.3)\n",
        "axes[0].set_title('Q(s, a) — действия из буфера')\n",
        "axes[0].set_xlabel('Q-value')\n",
        "axes[0].set_ylabel('Частота')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].hist(q_min_policy, bins=100, alpha=0.7, color='tab:red', edgecolor='black', linewidth=0.3)\n",
        "axes[1].set_title('Q(s, π(s)) — действия актора')\n",
        "axes[1].set_xlabel('Q-value')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Распределение Q-значений (min(Q1, Q2))', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q(s, π(s)) как функция error\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(buf_obs[:, 0], q_min_policy, alpha=0.05, s=1, color='tab:red')\n",
        "plt.xlabel('error (norm)')\n",
        "plt.ylabel('Q(s, π(s))')\n",
        "plt.title('Q-значение политики актора в зависимости от ошибки')\n",
        "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5, alpha=0.5)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap Q(s, π(s)) на сетке error × error_prev\n",
        "with torch.no_grad():\n",
        "    grid_actions, _ = actor(obs_tensor)\n",
        "    q1_grid, _ = critic1(obs_tensor, grid_actions)\n",
        "    q2_grid, _ = critic2(obs_tensor, grid_actions)\n",
        "    q_grid = torch.min(q1_grid, q2_grid).numpy().reshape(grid_n, grid_n)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "im = plt.imshow(q_grid, extent=[-1, 1, -1, 1], origin='lower', aspect='auto', cmap='viridis')\n",
        "plt.colorbar(im, label='Q(s, π(s))')\n",
        "plt.xlabel('error (norm)')\n",
        "plt.ylabel('error_prev (norm)')\n",
        "plt.title('Q-значение политики: Q(s, π(s)) при error_prev_prev=0')\n",
        "plt.grid(False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nn-laser-stabilizer",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
