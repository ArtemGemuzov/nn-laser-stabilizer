{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51985726",
   "metadata": {},
   "source": [
    "# Эксперимент по обучению с использованием сохраненных данных из буфера воспроизведения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from torchrl.data import LazyTensorStorage, TensorDictReplayBuffer\n",
    "\n",
    "def make_buffer(config):\n",
    "    buffer = TensorDictReplayBuffer(\n",
    "        batch_size=1,\n",
    "        storage=LazyTensorStorage(max_size=config.data.buffer_size),\n",
    "    )\n",
    "    return buffer\n",
    "\n",
    "EXPERIMENT_NAME = \"td3_train_real_async\"\n",
    "EXPERIMENT_DATE = \"2025-09-30\"\n",
    "EXPERIMENT_TIME = \"13-08-38\"\n",
    "\n",
    "PATH_TO_EXP_DIR = Path(f\"../experiments/{EXPERIMENT_NAME}/{EXPERIMENT_DATE}_{EXPERIMENT_TIME}\")\n",
    "\n",
    "ENV_LOG_DIR = PATH_TO_EXP_DIR / \"env_logs\"\n",
    "TRAIN_LOG_DIR = PATH_TO_EXP_DIR / \"train_logs\"\n",
    "\n",
    "config_path = PATH_TO_EXP_DIR / \".hydra\" / \"config.yaml\"\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "\n",
    "scan_buffer = make_buffer(config)\n",
    "\n",
    "buffer_path = PATH_TO_EXP_DIR / \"saved_data/replay_buffer.pkl\"\n",
    "scan_buffer.load(buffer_path) \n",
    "\n",
    "print(f\"Replay buffer загружен. Размер: {len(scan_buffer)} сэмплов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "batch = scan_buffer.sample(batch_size)\n",
    "print(f\"Сэмплирован batch из {len(batch)} сэмплов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cfe1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = batch[0]\n",
    "\n",
    "print(\"Поля сэмпла:\", sample.keys())\n",
    "\n",
    "print(\"\\nObservation:\", sample[\"observation\"])\n",
    "print(\"Next Observation:\", sample[\"next\"][\"observation\"])\n",
    "print(\"Action:\", sample[\"action\"])\n",
    "print(\"Reward:\", sample[\"next\"][\"reward\"])\n",
    "print(\"Done:\", sample[\"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_buffer = make_buffer(config)\n",
    "scan_buffer._batch_size = 1\n",
    "seen = set()\n",
    "\n",
    "for sample in scan_buffer:\n",
    "    action = sample[\"action\"]\n",
    "    \n",
    "    if (action[0] > 0.9).any() or (action[0] < -0.9).any():\n",
    "        continue\n",
    "\n",
    "    obs_tuple = tuple(sample[\"observation\"].flatten().tolist())\n",
    "    action_tuple = tuple(action.flatten().tolist())\n",
    "    key = (obs_tuple, action_tuple)\n",
    "    \n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "\n",
    "    observation = sample[\"observation\"]\n",
    "\n",
    "    observation[0][0] = observation[0][0] / 125.0\n",
    "    observation[0][1] = observation[0][1] / 10.0  \n",
    "    sample[\"observation\"] = observation\n",
    "\n",
    "    next_observation = sample[\"next\"][\"observation\"].clone()\n",
    "    next_observation[0][0] = next_observation[0][0] / 125.0\n",
    "    next_observation[0][1] = next_observation[0][1] / 10.0\n",
    "    sample[\"next\"][\"observation\"] = next_observation\n",
    "    \n",
    "    filtered_buffer.add(sample)\n",
    "\n",
    "    if (len(filtered_buffer) > 2500):\n",
    "        break\n",
    "\n",
    "print(f\"Filtered buffer создан. Размер: {len(filtered_buffer)} сэмплов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61203402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from nn_laser_stabilizer.envs.utils import make_specs\n",
    "from nn_laser_stabilizer.logging.utils import set_seeds\n",
    "from nn_laser_stabilizer.agents.td3 import (\n",
    "    make_td3_agent,\n",
    "    make_loss_module,\n",
    "    make_optimizers,\n",
    "    make_target_updater,\n",
    "    train_step,\n",
    "    warmup_from_specs\n",
    ")\n",
    "from nn_laser_stabilizer.envs.normalization import denormalize_kp, denormalize_ki, denormalize_kd\n",
    "\n",
    "set_seeds(config.seed)\n",
    "\n",
    "specs = make_specs(config.env.bounds)\n",
    "action_spec = specs[\"action\"]\n",
    "observation_spec = specs[\"observation\"]\n",
    "\n",
    "config.agent.learning_rate_actor = 1e-4\n",
    "config.agent.learning_rate_critic = 1e-4\n",
    "config.agent.noise_level = 0.2\n",
    "config.agent.noise_clip = 0.5\n",
    "\n",
    "actor, qvalue = make_td3_agent(config, observation_spec, action_spec)\n",
    "warmup_from_specs(observation_spec, action_spec, actor, qvalue)\n",
    "\n",
    "loss_module = make_loss_module(config, actor, qvalue, action_spec)\n",
    "optimizer_actor, optimizer_critic = make_optimizers(config, loss_module)\n",
    "target_net_updater = make_target_updater(config, loss_module)\n",
    "\n",
    "train_config = config.train\n",
    "\n",
    "total_train_steps = 0\n",
    "recent_qvalue_losses = deque(maxlen=train_config.update_to_data)\n",
    "recent_actor_losses = deque(maxlen=train_config.update_to_data // train_config.update_actor_freq)\n",
    "\n",
    "print(\"Training process initiated\")\n",
    "\n",
    "try:\n",
    "    for _ in range(1000):\n",
    "        try:\n",
    "            for i in range(train_config.update_to_data):\n",
    "                batch = filtered_buffer.sample(train_config.batch_size)\n",
    "                update_actor = i % train_config.update_actor_freq == 0\n",
    "                loss_qvalue_val, loss_actor_val = train_step(\n",
    "                    batch, loss_module, optimizer_actor, optimizer_critic,\n",
    "                    target_net_updater, update_actor\n",
    "                )\n",
    "\n",
    "                recent_qvalue_losses.append(loss_qvalue_val)\n",
    "                if loss_actor_val is not None:\n",
    "                    recent_actor_losses.append(loss_actor_val)\n",
    "\n",
    "            avg_qvalue_loss = sum(recent_qvalue_losses) / len(recent_qvalue_losses)\n",
    "            avg_actor_loss = sum(recent_actor_losses) / len(recent_actor_losses)\n",
    "            print(f\"step={total_train_steps} Loss/Critic={avg_qvalue_loss} Loss/Actor={avg_actor_loss}\")\n",
    "\n",
    "            total_train_steps += 1\n",
    "\n",
    "            actor.eval()\n",
    "            with torch.no_grad():\n",
    "                action = actor(batch[0])\n",
    "\n",
    "            action_list = action[\"action\"].tolist()[0]\n",
    "            print(f\"kp = {denormalize_kp(action_list[0])} ki = {denormalize_ki(action_list[1])} kd = {denormalize_kd(action_list[2])}\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Training interrupted by user.\")\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"Error while training: {ex}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Training finished\")\n",
    "    print(f\"Final buffer size: {len(filtered_buffer)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3222fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    sample = filtered_buffer.sample(1)[0]  \n",
    "    sample_action = sample[\"action\"]\n",
    "\n",
    "    actor.eval()\n",
    "    with torch.no_grad():\n",
    "        action = actor(sample)\n",
    "\n",
    "    print(\"Observation:\", sample[\"observation\"])\n",
    "    print(\"Action (from buffer):\", sample_action)\n",
    "    print(\"Action predicted by actor:\", action[\"action\"])\n",
    "\n",
    "    action_list = action[\"action\"].tolist()[0]\n",
    "    print(f\"kp = {denormalize_kp(action_list[0])} ki = {denormalize_ki(action_list[1])} kd = {denormalize_kd(action_list[2])}\")\n",
    "\n",
    "    qvalue.eval()\n",
    "    with torch.no_grad():\n",
    "        q_pred = qvalue(sample)  \n",
    "\n",
    "    print(\"Predicted Q-value:\", q_pred[\"state_action_value\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58dfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mean_list = []\n",
    "error_std_list = []\n",
    "kp_list = []\n",
    "ki_list = []\n",
    "kd_list = []\n",
    "reward_list = []\n",
    "done_list = []\n",
    "\n",
    "for i in range(len(scan_buffer)):\n",
    "    sample = scan_buffer[i]\n",
    "    \n",
    "    error_mean_list.append(sample[\"observation\"][0].cpu())\n",
    "    error_std_list.append(sample[\"observation\"][1].cpu())\n",
    "    \n",
    "    kp_list.append(sample[\"action\"][0].cpu())\n",
    "    ki_list.append(sample[\"action\"][1].cpu())\n",
    "    kd_list.append(sample[\"action\"][2].cpu())\n",
    "    \n",
    "    reward_list.append(sample[\"next\"][\"reward\"].cpu())\n",
    "    done_list.append(sample[\"done\"].cpu())\n",
    "\n",
    "error_mean = torch.stack(error_mean_list)\n",
    "error_std = torch.stack(error_std_list)\n",
    "kp = torch.stack(kp_list)\n",
    "ki = torch.stack(ki_list)\n",
    "kd = torch.stack(kd_list)\n",
    "reward = torch.stack(reward_list)\n",
    "done = torch.stack(done_list)\n",
    "\n",
    "def stats(tensor, name):\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(\"Min:\", tensor.min().item())\n",
    "    print(\"Max:\", tensor.max().item())\n",
    "    print(\"Mean:\", tensor.mean().item())\n",
    "    print(\"Std:\", tensor.std().item())\n",
    "    print()\n",
    "\n",
    "stats(error_mean, \"Error Mean (obs[0])\")\n",
    "stats(error_std, \"Error Std (obs[1])\")\n",
    "stats(kp, \"KP\")\n",
    "stats(ki, \"KI\")\n",
    "stats(kd, \"KD\")\n",
    "stats(reward, \"Reward\")\n",
    "stats(done.float(), \"Done flags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d0aae",
   "metadata": {},
   "source": [
    "## А теперь обучение на ступенчатых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dc7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torchrl.data import LazyTensorStorage, TensorDictReplayBuffer\n",
    "from tensordict import TensorDict\n",
    "from nn_laser_stabilizer.envs.normalization import (\n",
    "    normalize_kp, normalize_ki, normalize_kd,\n",
    "    denormalize_kp, denormalize_ki, denormalize_kd\n",
    ")\n",
    "from nn_laser_stabilizer.envs.reward import ExponentialErrorReward\n",
    "\n",
    "EXPERIMENT_NAME = \"pid_grid_scan\"\n",
    "EXPERIMENT_DATE = \"2025-09-26\"\n",
    "EXPERIMENT_TIME = \"16-48-36\"\n",
    "\n",
    "PATH_TO_EXP_DIR = Path(f\"../experiments/{EXPERIMENT_NAME}/{EXPERIMENT_DATE}/{EXPERIMENT_TIME}\")\n",
    "LOG_DIR = PATH_TO_EXP_DIR / \"logs\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_pid_logfile(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Читает лог PID-контроллера из файла и возвращает DataFrame.\n",
    "    Для каждого step объединяет send и recv в одну строку.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"(\\w+)=([0-9.+-eE]+)\")\n",
    "    steps = defaultdict(dict)\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            parts = line.split(maxsplit=3)\n",
    "            step = int(parts[0].split(\"=\")[1])\n",
    "            timestamp = float(parts[1].split(\"=\")[1])\n",
    "            direction = parts[2]  # send / recv\n",
    "\n",
    "            steps[step][f\"time_{direction}\"] = timestamp\n",
    "\n",
    "            if len(parts) > 3:\n",
    "                for key, value in pattern.findall(parts[3]):\n",
    "                    steps[step][key] = float(value)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(steps, orient=\"index\").reset_index()\n",
    "    df = df.rename(columns={\"index\": \"step\"})\n",
    "    return df.sort_values(\"step\").reset_index(drop=True)\n",
    "\n",
    "log_df = parse_pid_logfile(LOG_DIR / \"log.txt\")\n",
    "\n",
    "SETPOINT = 1200\n",
    "\n",
    "log_df = log_df[log_df[\"step\"] >= 1000].copy()\n",
    "\n",
    "log_df[\"error\"] = log_df[\"process_variable\"] - SETPOINT\n",
    "\n",
    "BLOCK_SIZE = 2000  \n",
    "LAST_STEPS = 200  \n",
    "\n",
    "scan_buffer = TensorDictReplayBuffer(\n",
    "    batch_size=1,\n",
    "    storage=LazyTensorStorage(max_size=10000),\n",
    ")\n",
    "\n",
    "reward_func = ExponentialErrorReward(k=20)\n",
    "\n",
    "print(f\"Всего записей в логе: {len(log_df)}\")\n",
    "print(f\"Шаги от {log_df['step'].min()} до {log_df['step'].max()}\")\n",
    "\n",
    "total_samples = 0\n",
    "block_count = 0\n",
    "\n",
    "for start_step in range(log_df['step'].min(), log_df['step'].max(), BLOCK_SIZE):\n",
    "    end_step = start_step + BLOCK_SIZE\n",
    "    \n",
    "    block_df = log_df[(log_df['step'] >= start_step) & (log_df['step'] < end_step)].copy()\n",
    "    \n",
    "    if len(block_df) < LAST_STEPS:\n",
    "        continue\n",
    "    \n",
    "    last_steps_df = block_df.tail(LAST_STEPS).copy()\n",
    "    \n",
    "    error_mean = last_steps_df[\"error\"].mean()\n",
    "    error_std = last_steps_df[\"error\"].std()\n",
    "    \n",
    "    observation = torch.tensor([\n",
    "        error_mean / 250.0, \n",
    "        error_std / 200.0\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    avg_kp = last_steps_df[\"kp\"].iloc[0]  \n",
    "    avg_ki = last_steps_df[\"ki\"].iloc[0]  \n",
    "    avg_kd = last_steps_df[\"kd\"].iloc[0]  \n",
    "    \n",
    "    action = torch.tensor([\n",
    "        normalize_kp(avg_kp),\n",
    "        normalize_ki(avg_ki),\n",
    "        normalize_kd(avg_kd)\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    rewards = [reward_func(pv, SETPOINT) for pv in last_steps_df[\"process_variable\"]]\n",
    "    avg_reward = sum(rewards) / len(rewards)\n",
    "    reward_tensor = torch.tensor([avg_reward], dtype=torch.float32)\n",
    "    \n",
    "    next_block_start = end_step\n",
    "    next_block_df = log_df[(log_df['step'] >= next_block_start) & (log_df['step'] < next_block_start + BLOCK_SIZE)].copy()\n",
    "    \n",
    "    if len(next_block_df) >= LAST_STEPS:\n",
    "        next_last_steps_df = next_block_df.tail(LAST_STEPS).copy()\n",
    "        next_error_mean = next_last_steps_df[\"error\"].mean()\n",
    "        next_error_std = next_last_steps_df[\"error\"].std()\n",
    "            \n",
    "        next_observation = torch.tensor([\n",
    "            next_error_mean / 250.0,\n",
    "            next_error_std / 200.0\n",
    "        ], dtype=torch.float32)\n",
    "        done = torch.tensor([True], dtype=torch.bool)\n",
    "    else:\n",
    "        next_observation = observation.clone()\n",
    "        done = torch.tensor([True], dtype=torch.bool)\n",
    "    \n",
    "    sample = TensorDict({\n",
    "        \"observation\": observation.unsqueeze(0),\n",
    "        \"action\": action.unsqueeze(0),\n",
    "        \"next\": TensorDict({\n",
    "            \"observation\": next_observation.unsqueeze(0),\n",
    "            \"reward\": reward_tensor.unsqueeze(0),\n",
    "            \"done\": done.unsqueeze(0)\n",
    "        }, batch_size=1)\n",
    "    }, batch_size=1)\n",
    "    \n",
    "    scan_buffer.add(sample)\n",
    "    total_samples += 1\n",
    "    block_count += 1\n",
    "    \n",
    "    if block_count % 10 == 0:\n",
    "        print(f\"Обработано блоков: {block_count}, образцов в буфере: {len(scan_buffer)}\")\n",
    "\n",
    "print(f\"\\nИтоговая статистика:\")\n",
    "print(f\"Обработано блоков: {block_count}\")\n",
    "print(f\"Образцов в буфере: {len(scan_buffer)}\")\n",
    "\n",
    "if len(scan_buffer) > 0:\n",
    "    sample = scan_buffer.sample(1)[0]\n",
    "    print(f\"\\nПример образца:\")\n",
    "    print(f\"Observation: {sample['observation']}\")\n",
    "    print(f\"Action: {sample['action']}\")\n",
    "    print(f\"Reward: {sample['next']['reward']}\")\n",
    "    print(f\"Done: {sample['next']['done']}\")\n",
    "    \n",
    "    action_list = sample['action'].squeeze().tolist()\n",
    "    print(f\"Денормализованные действия:\")\n",
    "    print(f\"KP: {denormalize_kp(action_list[0]):.4f}\")\n",
    "    print(f\"KI: {denormalize_ki(action_list[1]):.4f}\")\n",
    "    print(f\"KD: {denormalize_kd(action_list[2]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from nn_laser_stabilizer.envs.utils import make_specs\n",
    "from nn_laser_stabilizer.logging.utils import set_seeds\n",
    "from nn_laser_stabilizer.agents.td3 import (\n",
    "    make_td3_agent,\n",
    "    make_loss_module,\n",
    "    make_optimizers,\n",
    "    make_target_updater,\n",
    "    train_step,\n",
    "    warmup_from_specs\n",
    ")\n",
    "from nn_laser_stabilizer.envs.normalization import denormalize_kp, denormalize_ki, denormalize_kd\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "EXPERIMENT_NAME = \"td3_train_real_async\"\n",
    "EXPERIMENT_DATE = \"2025-09-30\"\n",
    "EXPERIMENT_TIME = \"13-08-38\"\n",
    "\n",
    "PATH_TO_EXP_DIR = Path(f\"../experiments/{EXPERIMENT_NAME}/{EXPERIMENT_DATE}_{EXPERIMENT_TIME}\")\n",
    "\n",
    "ENV_LOG_DIR = PATH_TO_EXP_DIR / \"env_logs\"\n",
    "TRAIN_LOG_DIR = PATH_TO_EXP_DIR / \"train_logs\"\n",
    "\n",
    "config_path = PATH_TO_EXP_DIR / \".hydra\" / \"config.yaml\"\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "\n",
    "set_seeds(config.seed)\n",
    "\n",
    "specs = make_specs(config.env.bounds)\n",
    "action_spec = specs[\"action\"]\n",
    "observation_spec = specs[\"observation\"]\n",
    "\n",
    "config.agent.learning_rate_actor = 1e-4\n",
    "config.agent.learning_rate_critic = 1e-4\n",
    "config.agent.noise_level = 0.2\n",
    "config.agent.noise_clip = 0.5\n",
    "\n",
    "actor, qvalue = make_td3_agent(config, observation_spec, action_spec)\n",
    "warmup_from_specs(observation_spec, action_spec, actor, qvalue)\n",
    "\n",
    "loss_module = make_loss_module(config, actor, qvalue, action_spec)\n",
    "optimizer_actor, optimizer_critic = make_optimizers(config, loss_module)\n",
    "target_net_updater = make_target_updater(config, loss_module)\n",
    "\n",
    "train_config = config.train\n",
    "\n",
    "total_train_steps = 0\n",
    "recent_qvalue_losses = deque(maxlen=train_config.update_to_data)\n",
    "recent_actor_losses = deque(maxlen=train_config.update_to_data // train_config.update_actor_freq)\n",
    "\n",
    "print(\"Training process initiated\")\n",
    "\n",
    "try:\n",
    "    for _ in range(1000):\n",
    "        try:\n",
    "            for i in range(train_config.update_to_data):\n",
    "                batch = scan_buffer.sample(train_config.batch_size)\n",
    "                update_actor = i % train_config.update_actor_freq == 0\n",
    "                loss_qvalue_val, loss_actor_val = train_step(\n",
    "                    batch, loss_module, optimizer_actor, optimizer_critic,\n",
    "                    target_net_updater, update_actor\n",
    "                )\n",
    "\n",
    "                recent_qvalue_losses.append(loss_qvalue_val)\n",
    "                if loss_actor_val is not None:\n",
    "                    recent_actor_losses.append(loss_actor_val)\n",
    "\n",
    "            avg_qvalue_loss = sum(recent_qvalue_losses) / len(recent_qvalue_losses)\n",
    "            avg_actor_loss = sum(recent_actor_losses) / len(recent_actor_losses)\n",
    "            print(f\"step={total_train_steps} Loss/Critic={avg_qvalue_loss} Loss/Actor={avg_actor_loss}\")\n",
    "\n",
    "            total_train_steps += 1\n",
    "\n",
    "            actor.eval()\n",
    "            with torch.no_grad():\n",
    "                action = actor(batch[0])\n",
    "\n",
    "            action_list = action[\"action\"].tolist()[0]\n",
    "            print(f\"kp = {denormalize_kp(action_list[0])} ki = {denormalize_ki(action_list[1])} kd = {denormalize_kd(action_list[2])}\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Training interrupted by user.\")\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"Error while training: {ex}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Training finished\")\n",
    "    print(f\"Final buffer size: {len(scan_buffer)} samples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn-laser-stabilizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
