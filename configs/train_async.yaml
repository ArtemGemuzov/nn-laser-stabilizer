experiment_name: td3_train_async
seed: 42 
output_dir: # Должно быть пустым

defaults:
  - agent: agent
  - _self_

serial: # CHECK
  port: COM3 
  baudrate: 115200
  timeout: 0.1
  use_mock: false
  log_connection: true

env:
  name: real 
  setpoint: 1200 # CHECK

  process_variable_max: 10230.0
  control_output_max: 4095.0
  
  default_kp: 3.5
  default_ki: 11.0
  default_kd: 0.002
  
  default_min_control: 0
  default_max_control: 4095
  
  kp_min: 0.0
  ki_min: 0.0
  kd_min: 0.0
  kp_max: 17.5
  ki_max: 55
  kd_max: 0.01

  reward:
    name: exponential # exponential absolute relative
    args:
      k: 20.0

  bounds:
    action:
      low: [-1.0, -1.0]
      high: [1.0, 1.0]
    observation:
      low: [-inf, 0.0, -1.0, -1.0] 
      high: [inf, inf, 1.0, 1.0]
    reward:
      low: [-1.0]
      high: [1.0]

  force_min_value: 2000
  force_max_value: 2500
  
  default_min: 0
  default_max: 4095
  
  lower_force_condition_threshold: 200
  upper_force_condition_threshold: 4000
  enforcement_steps: 1000
  
  warmup_steps: 1000
  pretrain_blocks: 2651 # TODO: рассчитывать через другие параметры (2500 + 1 + 150)
  block_size: 200
  burn_in_steps: 100  

hydra:
  run:
    dir: experiments/${experiment_name}/${now:%Y-%m-%d}_${now:%H-%M-%S}

data:
  frames_per_batch: 1
  total_frames: 1_000_000_000

  buffer_size: 100_000
  min_data_for_training: 2501

train:
  batch_size: 64
  
  update_to_data: 10
  update_actor_freq: 2 

  total_train_steps: 1_000_000_000 # Обучение идет до прерывания через CTRL + C