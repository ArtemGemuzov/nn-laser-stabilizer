{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Анализ эксперимента с train_from_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from nn_laser_stabilizer.config.config import load_config\n",
        "\n",
        "EXPERIMENT_DATE_TIME = \"YYYY-MM-DD_HH-MM-SS\"\n",
        "EXPERIMENT_NAME = \"train_from_csv\"\n",
        "\n",
        "EXPERIMENT_DIR_PATH = Path(f\"../experiments/{EXPERIMENT_NAME}/{EXPERIMENT_DATE_TIME}\")\n",
        "\n",
        "CONFIG_PATH = EXPERIMENT_DIR_PATH / \"config.yaml\"\n",
        "config = load_config(CONFIG_PATH)\n",
        "\n",
        "base_config_path = Path(f\"../configs/{config.base_config}.yaml\")\n",
        "base_config = load_config(base_config_path)\n",
        "\n",
        "print(f\"Эксперимент: {config.experiment_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Анализ процесса обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def parse_train_logs(file_path):\n",
        "    step_pattern = re.compile(\n",
        "        r\"\\[TRAIN\\]\\s+\"\n",
        "        r\"step:\\s+\"\n",
        "        r\"(actor_loss=(?P<actor_loss>-?\\d+\\.\\d+)\\s+)?\"\n",
        "        r\"buffer_size=(?P<buffer_size>\\d+)\\s+\"\n",
        "        r\"loss_q1=(?P<loss_q1>-?\\d+\\.\\d+)\\s+\"\n",
        "        r\"loss_q2=(?P<loss_q2>-?\\d+\\.\\d+)\\s+\"\n",
        "        r\"step=(?P<step>\\d+)\\s+\"\n",
        "        r\"time=(?P<time>-?\\d+\\.\\d+)\"\n",
        "    )\n",
        "    \n",
        "    rows = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            match = step_pattern.match(line)\n",
        "            if match:\n",
        "                actor_loss = match.group('actor_loss')\n",
        "                rows.append({\n",
        "                    'step': int(match.group('step')),\n",
        "                    'loss_q1': float(match.group('loss_q1')),\n",
        "                    'loss_q2': float(match.group('loss_q2')),\n",
        "                    'actor_loss': float(actor_loss) if actor_loss else np.nan,\n",
        "                    'buffer_size': int(match.group('buffer_size'))\n",
        "                })\n",
        "    \n",
        "    return pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_LOG_PATH = EXPERIMENT_DIR_PATH / \"train_logs\" / \"train.log\"\n",
        "loss_df = parse_train_logs(TRAIN_LOG_PATH)\n",
        "print(f\"Загружено {len(loss_df)} записей из логов обучения\")\n",
        "print(f\"Диапазон шагов обучения: {loss_df['step'].min()} - {loss_df['step'].max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
        "\n",
        "axes[0].plot(loss_df['step'], loss_df['loss_q1'], 'b-', alpha=0.7, label='Q1 Loss')\n",
        "axes[0].set_title('Q1 Loss')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(loss_df['step'], loss_df['loss_q2'], 'g-', alpha=0.7, label='Q2 Loss')\n",
        "axes[1].set_title('Q2 Loss')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(loss_df['step'], loss_df['loss_q1'] + loss_df['loss_q2'], 'r--', alpha=0.7, label='Sum (Q1 + Q2)')\n",
        "axes[2].set_title('Sum (Q1 + Q2)')\n",
        "axes[2].set_xlabel('Step')\n",
        "axes[2].set_ylabel('Loss')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "actor_loss_df = loss_df[loss_df['actor_loss'].notna()]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.plot(actor_loss_df['step'], actor_loss_df['actor_loss'], 'r-', alpha=0.7)\n",
        "plt.title('Actor Loss')\n",
        "\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(loss_df['step'], loss_df['buffer_size'], 'm-', alpha=0.7)\n",
        "plt.title('Buffer Size')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Size')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Анализ работы обученной модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from nn_laser_stabilizer.data.replay_buffer import ReplayBuffer\n",
        "from nn_laser_stabilizer.model.actor import load_actor_from_path\n",
        "from nn_laser_stabilizer.config.types import NetworkType\n",
        "\n",
        "ACTOR_PATH = EXPERIMENT_DIR_PATH / \"models\" / \"actor.pth\"  \n",
        "BUFFER_PATH = EXPERIMENT_DIR_PATH / \"data\" / \"replay_buffer.pth\"\n",
        "\n",
        "buffer = ReplayBuffer.load(BUFFER_PATH)\n",
        "print(f\"ReplayBuffer loaded. Size: {len(buffer)} / capacity={buffer.capacity}\")\n",
        "\n",
        "actor_path = ACTOR_PATH.resolve()\n",
        "if not actor_path.exists():\n",
        "    raise FileNotFoundError(f\"Actor model not found: {actor_path}\")\n",
        "\n",
        "print(f\"\\nLoading actor from: {actor_path}\")\n",
        "\n",
        "network_type_str = base_config.network.type\n",
        "network_type = NetworkType(network_type_str)\n",
        "\n",
        "actor = load_actor_from_path(actor_path, network_type)\n",
        "actor.eval()\n",
        "\n",
        "print(f\"Actor loaded successfully (type: {network_type_str})\")\n",
        "\n",
        "buffer_size = len(buffer)\n",
        "observations = buffer.observations[:buffer_size]\n",
        "true_actions = buffer.actions[:buffer_size]\n",
        "rewards = buffer.rewards[:buffer_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Analyzing {buffer_size} transitions...\")\n",
        "\n",
        "predicted_actions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    if network_type == NetworkType.LSTM:\n",
        "        hidden_state = None\n",
        "        for i in range(buffer_size):\n",
        "            obs = observations[i]\n",
        "            action, options = actor.act(obs, {'hidden_state': hidden_state})\n",
        "            hidden_state = options.get('hidden_state')\n",
        "            predicted_actions.append(action)\n",
        "        \n",
        "        predicted_actions = torch.stack(predicted_actions, dim=0)\n",
        "    else:\n",
        "        batch_size = 1024\n",
        "        for i in range(0, buffer_size, batch_size):\n",
        "            end_idx = min(i + batch_size, buffer_size)\n",
        "            batch_obs = observations[i:end_idx]\n",
        "            batch_actions, _ = actor.act(batch_obs)\n",
        "            predicted_actions.append(batch_actions)\n",
        "        \n",
        "        predicted_actions = torch.cat(predicted_actions, dim=0)\n",
        "\n",
        "print(f\"Predictions completed. Shape: {predicted_actions.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse = torch.mean((predicted_actions - true_actions) ** 2).item()\n",
        "mae = torch.mean(torch.abs(predicted_actions - true_actions)).item()\n",
        "\n",
        "action_dim = true_actions.shape[1]\n",
        "mse_per_dim = []\n",
        "mae_per_dim = []\n",
        "for dim in range(action_dim):\n",
        "    mse_dim = torch.mean((predicted_actions[:, dim] - true_actions[:, dim]) ** 2).item()\n",
        "    mae_dim = torch.mean(torch.abs(predicted_actions[:, dim] - true_actions[:, dim])).item()\n",
        "    mse_per_dim.append(mse_dim)\n",
        "    mae_per_dim.append(mae_dim)\n",
        "\n",
        "mean_reward = torch.mean(rewards).item()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Analysis Results:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total transitions analyzed: {buffer_size}\")\n",
        "print(f\"Mean reward: {mean_reward:.6f}\")\n",
        "print(\"\")\n",
        "print(\"Action prediction metrics:\")\n",
        "print(f\"  MSE (Mean Squared Error): {mse:.6f}\")\n",
        "print(f\"  MAE (Mean Absolute Error): {mae:.6f}\")\n",
        "\n",
        "if action_dim > 1:\n",
        "    print(\"  Per-dimension metrics:\")\n",
        "    for dim in range(action_dim):\n",
        "        print(f\"    Dim {dim}: MSE={mse_per_dim[dim]:.6f}, MAE={mae_per_dim[dim]:.6f}\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "obs_error = observations[:, 0].numpy()\n",
        "obs_control_norm = observations[:, 1].numpy()\n",
        "true_actions_np = true_actions[:, 0].numpy()\n",
        "predicted_actions_np = predicted_actions[:, 0].numpy()\n",
        "rewards_np = rewards[:, 0].numpy()\n",
        "action_errors = predicted_actions_np - true_actions_np\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "axes[0, 0].scatter(true_actions_np, predicted_actions_np, alpha=0.5, s=1)\n",
        "axes[0, 0].plot([true_actions_np.min(), true_actions_np.max()], \n",
        "                [true_actions_np.min(), true_actions_np.max()], \n",
        "                'r--', lw=2, label='Perfect prediction')\n",
        "axes[0, 0].set_xlabel('True Action')\n",
        "axes[0, 0].set_ylabel('Predicted Action')\n",
        "axes[0, 0].set_title('True vs Predicted Actions')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].plot(action_errors, alpha=0.7, linewidth=0.5)\n",
        "axes[0, 1].axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
        "axes[0, 1].set_xlabel('Transition Index')\n",
        "axes[0, 1].set_ylabel('Action Error (Predicted - True)')\n",
        "axes[0, 1].set_title('Action Prediction Error Over Time')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 0].hist(action_errors, bins=50, alpha=0.7, edgecolor='black')\n",
        "axes[1, 0].axvline(x=0, color='r', linestyle='--', linewidth=1)\n",
        "axes[1, 0].set_xlabel('Action Error')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title('Distribution of Action Errors')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].plot(rewards_np, alpha=0.7, linewidth=0.5)\n",
        "axes[1, 1].axhline(y=mean_reward, color='r', linestyle='--', linewidth=2, \n",
        "                   label=f'Mean reward: {mean_reward:.4f}')\n",
        "axes[1, 1].set_xlabel('Transition Index')\n",
        "axes[1, 1].set_ylabel('Reward')\n",
        "axes[1, 1].set_title('Reward Over Time')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "axes[0].scatter(obs_error, action_errors, alpha=0.5, s=1)\n",
        "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
        "axes[0].set_xlabel('Observation Error (setpoint - process_variable)')\n",
        "axes[0].set_ylabel('Action Prediction Error')\n",
        "axes[0].set_title('Action Error vs Observation Error')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].scatter(obs_control_norm, action_errors, alpha=0.5, s=1)\n",
        "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
        "axes[1].set_xlabel('Observation Control Output (normalized)')\n",
        "axes[1].set_ylabel('Action Prediction Error')\n",
        "axes[1].set_title('Action Error vs Control Output in Observation')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'observation_error': obs_error,\n",
        "    'observation_control_output_norm': obs_control_norm,\n",
        "    'true_action': true_actions_np,\n",
        "    'predicted_action': predicted_actions_np,\n",
        "    'action_error': action_errors,\n",
        "    'reward': rewards_np,\n",
        "})\n",
        "\n",
        "output_path = Path(\"analysis_results.csv\")\n",
        "results_df.to_csv(output_path, index=False)\n",
        "print(f\"\\nDetailed results saved to: {output_path}\")\n",
        "\n",
        "print(\"\\nStatistics:\")\n",
        "print(results_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 6))\n",
        "\n",
        "sample_size = len(true_actions_np) \n",
        "indices = np.arange(sample_size)\n",
        "\n",
        "ax.plot(indices, true_actions_np[:sample_size], \n",
        "        label='True Action', \n",
        "        alpha=0.7, \n",
        "        linewidth=0.8,\n",
        "        color='blue')\n",
        "\n",
        "ax.plot(indices, predicted_actions_np[:sample_size], \n",
        "        label='Predicted Action (Model)', \n",
        "        alpha=0.7, \n",
        "        linewidth=0.8,\n",
        "        color='red')\n",
        "\n",
        "ax.set_xlabel('Transition Index (Time)', fontsize=12)\n",
        "ax.set_ylabel('Action Value', fontsize=12)\n",
        "ax.set_title('True Action vs Predicted Action Over Time', fontsize=14)\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "observations = buffer.observations[:buffer_size]   # shape: [N, obs_dim]\n",
        "actions      = buffer.actions[:buffer_size]        # shape: [N, act_dim]\n",
        "rewards      = buffer.rewards[:buffer_size]        # shape: [N, 1]\n",
        "\n",
        "# Предполагаем, что observation = [error, control_output_norm]\n",
        "obs_error         = observations[:, 0].numpy()\n",
        "obs_control_norm  = observations[:, 1].numpy()\n",
        "actions_np        = actions[:, 0].numpy()\n",
        "rewards_np        = rewards[:, 0].numpy()\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"error\": obs_error,\n",
        "    \"control_norm\": obs_control_norm,\n",
        "    \"action\": actions_np,\n",
        "    \"reward\": rewards_np,\n",
        "})\n",
        "print(\"\\nDataFrame head:\")\n",
        "display(df.head())\n",
        "print(\"\\nDataFrame describe:\")\n",
        "display(df.describe())\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "axes[0, 0].hist(df[\"error\"], bins=100, alpha=0.7, edgecolor=\"black\")\n",
        "axes[0, 0].set_title(\"Error distribution\")\n",
        "axes[0, 0].set_xlabel(\"error\")\n",
        "axes[0, 0].set_ylabel(\"count\")\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].hist(df[\"control_norm\"], bins=100, alpha=0.7, edgecolor=\"black\", color=\"orange\")\n",
        "axes[0, 1].set_title(\"Control (in observation) distribution\")\n",
        "axes[0, 1].set_xlabel(\"control_norm\")\n",
        "axes[0, 1].set_ylabel(\"count\")\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 0].hist(df[\"action\"], bins=100, alpha=0.7, edgecolor=\"black\", color=\"green\")\n",
        "axes[1, 0].set_title(\"Action distribution\")\n",
        "axes[1, 0].set_xlabel(\"action\")\n",
        "axes[1, 0].set_ylabel(\"count\")\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].hist(df[\"reward\"], bins=100, alpha=0.7, edgecolor=\"black\", color=\"purple\")\n",
        "axes[1, 1].set_title(\"Reward distribution\")\n",
        "axes[1, 1].set_xlabel(\"reward\")\n",
        "axes[1, 1].set_ylabel(\"count\")\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "axes[0].scatter(df[\"error\"], df[\"action\"], s=1, alpha=0.3)\n",
        "axes[0].set_xlabel(\"error\")\n",
        "axes[0].set_ylabel(\"action\")\n",
        "axes[0].set_title(\"Action vs Error\")\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].scatter(df[\"error\"], df[\"reward\"], s=1, alpha=0.3, color=\"red\")\n",
        "axes[1].set_xlabel(\"error\")\n",
        "axes[1].set_ylabel(\"reward\")\n",
        "axes[1].set_title(\"Reward vs Error\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].scatter(df[\"action\"], df[\"reward\"], s=1, alpha=0.3, color=\"green\")\n",
        "axes[2].set_xlabel(\"action\")\n",
        "axes[2].set_ylabel(\"reward\")\n",
        "axes[2].set_title(\"Reward vs Action\")\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "max_points = buffer_size\n",
        "idx = np.arange(max_points)\n",
        "\n",
        "fig, axes = plt.subplots(4, 1, figsize=(14, 10), sharex=True)\n",
        "\n",
        "axes[0].plot(idx, df[\"error\"].values[:max_points], linewidth=0.5)\n",
        "axes[0].set_ylabel(\"error\")\n",
        "axes[0].set_title(\"Error over time\")\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(idx, df[\"control_norm\"].values[:max_points], linewidth=0.5, color=\"orange\")\n",
        "axes[1].set_ylabel(\"control_norm\")\n",
        "axes[1].set_title(\"Control (in observation) over time\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(idx, df[\"action\"].values[:max_points], linewidth=0.5, color=\"green\")\n",
        "axes[2].set_ylabel(\"action\")\n",
        "axes[2].set_title(\"Action over time\")\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "axes[3].plot(idx, df[\"reward\"].values[:max_points], linewidth=0.5, color=\"purple\")\n",
        "axes[3].set_ylabel(\"reward\")\n",
        "axes[3].set_xlabel(\"transition index\")\n",
        "axes[3].set_title(\"Reward over time\")\n",
        "axes[3].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "corr = df.corr(numeric_only=True)\n",
        "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "plt.title(\"Correlation matrix (error, control_norm, action, reward)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nn-laser-stabilizer",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
