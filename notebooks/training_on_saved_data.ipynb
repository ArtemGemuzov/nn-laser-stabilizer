{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51985726",
   "metadata": {},
   "source": [
    "# Эксперимент по обучению с использованием сохраненных данных из буфера воспроизведения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from nn_laser_stabilizer.data.utils import make_buffer\n",
    "\n",
    "exp_dir = Path(\"../experiments/test/2025-09-29_15-58-48\")\n",
    "config_path = exp_dir / \".hydra\" / \"config.yaml\"\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "\n",
    "buffer = make_buffer(config)\n",
    "\n",
    "buffer_path = exp_dir / \"saved_data/replay_buffer.pkl\"\n",
    "buffer.load(buffer_path) \n",
    "\n",
    "print(f\"Replay buffer загружен. Размер: {len(buffer)} сэмплов\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "batch = buffer.sample(batch_size)\n",
    "print(f\"Сэмплирован batch из {len(batch)} сэмплов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cfe1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = batch[0]\n",
    "\n",
    "print(\"Поля сэмпла:\", sample.keys())\n",
    "\n",
    "print(\"\\nObservation:\", sample[\"observation\"])\n",
    "print(\"Action:\", sample[\"action\"])\n",
    "print(\"Reward:\", sample[\"next\"][\"reward\"])\n",
    "print(\"Next Observation:\", sample[\"next\"][\"observation\"])\n",
    "print(\"Done:\", sample[\"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_list = []\n",
    "action_list = []\n",
    "reward_list = []\n",
    "\n",
    "for i in range(len(buffer)):\n",
    "    sample = buffer[i]\n",
    "\n",
    "    # sample[\"done\"] = torch.tensor(True)\n",
    "    # sample[\"terminated\"] = torch.tensor(True)\n",
    "\n",
    "    obs_list.append(sample[\"observation\"].cpu())\n",
    "    action_list.append(sample[\"action\"].cpu())\n",
    "    reward_list.append(sample[\"next\"][\"reward\"].cpu())\n",
    "\n",
    "    buffer[i] = sample  \n",
    "\n",
    "obs_tensor = torch.stack(obs_list)\n",
    "action_tensor = torch.stack(action_list)\n",
    "reward_tensor = torch.stack(reward_list)\n",
    "\n",
    "obs_mean = obs_tensor.mean(0)\n",
    "obs_std = obs_tensor.std(0) + 1e-8\n",
    "\n",
    "# action_mean = action_tensor.mean(0)\n",
    "# action_std = action_tensor.std(0) + 1e-8\n",
    "\n",
    "reward_max = reward_tensor.abs().max()\n",
    "reward_min = reward_tensor.min()\n",
    "reward_max_val = reward_max if reward_max != 0 else 1.0\n",
    "\n",
    "for i in range(len(buffer)):\n",
    "    sample = buffer[i]\n",
    "\n",
    "    sample[\"observation\"] = (sample[\"observation\"] - obs_mean) / obs_std\n",
    "    sample[\"next\"][\"observation\"] = (sample[\"next\"][\"observation\"] - obs_mean) / obs_std\n",
    "\n",
    "    sample[\"next\"][\"reward\"] = sample[\"next\"][\"reward\"] / reward_max_val\n",
    "\n",
    "    buffer[i] = sample  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61203402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from nn_laser_stabilizer.envs.utils import make_specs\n",
    "from nn_laser_stabilizer.logging.utils import set_seeds\n",
    "from nn_laser_stabilizer.agents.td3 import (\n",
    "    make_td3_agent,\n",
    "    make_loss_module,\n",
    "    make_optimizers,\n",
    "    make_target_updater,\n",
    "    train_step,\n",
    "    warmup_from_specs\n",
    ")\n",
    "\n",
    "set_seeds(config.seed)\n",
    "\n",
    "# CHECK\n",
    "config.agent.gamma = 0.0\n",
    "\n",
    "specs = make_specs(config.env.bounds)\n",
    "action_spec = specs[\"action\"]\n",
    "observation_spec = specs[\"observation\"]\n",
    "\n",
    "actor, qvalue = make_td3_agent(config, observation_spec, action_spec)\n",
    "warmup_from_specs(observation_spec, action_spec, actor, qvalue)\n",
    "\n",
    "loss_module = make_loss_module(config, actor, qvalue, action_spec)\n",
    "optimizer_actor, optimizer_critic = make_optimizers(config, loss_module)\n",
    "target_net_updater = make_target_updater(config, loss_module)\n",
    "\n",
    "train_config = config.train\n",
    "\n",
    "total_train_steps = 0\n",
    "recent_qvalue_losses = deque(maxlen=train_config.update_to_data)\n",
    "recent_actor_losses = deque(maxlen=train_config.update_to_data // train_config.update_actor_freq)\n",
    "\n",
    "print(\"Training process initiated\")\n",
    "\n",
    "for i in range(len(buffer)):\n",
    "    sample = buffer[i]\n",
    "    sample[\"next\"][\"done\"] = torch.tensor(True)\n",
    "    sample[\"next\"][\"terminated\"] = torch.tensor(True) \n",
    "    buffer[i] = sample \n",
    "\n",
    "for _ in range(1000):\n",
    "    try:\n",
    "        for i in range(train_config.update_to_data):\n",
    "            batch = buffer.sample(train_config.batch_size)\n",
    "            update_actor = i % train_config.update_actor_freq == 0\n",
    "            loss_qvalue_val, loss_actor_val = train_step(\n",
    "                batch, loss_module, optimizer_actor, optimizer_critic,\n",
    "                target_net_updater, update_actor\n",
    "            )\n",
    "\n",
    "            recent_qvalue_losses.append(loss_qvalue_val)\n",
    "            if loss_actor_val is not None:\n",
    "                recent_actor_losses.append(loss_actor_val)\n",
    "\n",
    "        avg_qvalue_loss = sum(recent_qvalue_losses) / len(recent_qvalue_losses)\n",
    "        avg_actor_loss = sum(recent_actor_losses) / len(recent_actor_losses)\n",
    "        print(f\"step={total_train_steps} Loss/Critic={avg_qvalue_loss} Loss/Actor={avg_actor_loss}\")\n",
    "\n",
    "        total_train_steps += 1\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user.\")\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(f\"Error while training: {ex}\")\n",
    "\n",
    "    finally:\n",
    "        print(\"Training finished\")\n",
    "        print(f\"Final buffer size: {len(buffer)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3222fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    sample = buffer.sample(1)[0]  \n",
    "\n",
    "    actor.eval()\n",
    "    with torch.no_grad():\n",
    "        action = actor(sample)\n",
    "\n",
    "    print(\"Observation:\", sample[\"observation\"])\n",
    "    print(\"Action (from buffer):\", sample[\"action\"])\n",
    "    print(\"Action predicted by actor:\", action[\"action\"])\n",
    "\n",
    "    qvalue.eval()\n",
    "    with torch.no_grad():\n",
    "        q_pred = qvalue(sample)  \n",
    "\n",
    "    print(\"Predicted Q-value:\", q_pred[\"state_action_value\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58dfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mean_list = []\n",
    "error_std_list = []\n",
    "kp_list = []\n",
    "ki_list = []\n",
    "kd_list = []\n",
    "reward_list = []\n",
    "done_list = []\n",
    "\n",
    "for i in range(len(buffer)):\n",
    "    sample = buffer[i]\n",
    "    \n",
    "    error_mean_list.append(sample[\"observation\"][0].cpu())\n",
    "    error_std_list.append(sample[\"observation\"][1].cpu())\n",
    "    \n",
    "    kp_list.append(sample[\"action\"][0].cpu())\n",
    "    ki_list.append(sample[\"action\"][1].cpu())\n",
    "    kd_list.append(sample[\"action\"][2].cpu())\n",
    "    \n",
    "    reward_list.append(sample[\"next\"][\"reward\"].cpu())\n",
    "    done_list.append(sample[\"done\"].cpu())\n",
    "\n",
    "error_mean = torch.stack(error_mean_list)\n",
    "error_std = torch.stack(error_std_list)\n",
    "kp = torch.stack(kp_list)\n",
    "ki = torch.stack(ki_list)\n",
    "kd = torch.stack(kd_list)\n",
    "reward = torch.stack(reward_list)\n",
    "done = torch.stack(done_list)\n",
    "\n",
    "def stats(tensor, name):\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(\"Min:\", tensor.min().item())\n",
    "    print(\"Max:\", tensor.max().item())\n",
    "    print(\"Mean:\", tensor.mean().item())\n",
    "    print(\"Std:\", tensor.std().item())\n",
    "    print()\n",
    "\n",
    "stats(error_mean, \"Error Mean (obs[0])\")\n",
    "stats(error_std, \"Error Std (obs[1])\")\n",
    "stats(kp, \"KP\")\n",
    "stats(ki, \"KI\")\n",
    "stats(kd, \"KD\")\n",
    "stats(reward, \"Reward\")\n",
    "stats(done.float(), \"Done flags\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn-laser-stabilizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
