{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Анализ эксперимента train_from_buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nn_laser_stabilizer.config.config import load_config\n",
        "from nn_laser_stabilizer.paths import get_experiment_dir\n",
        "\n",
        "EXPERIMENT_DIR_PATH = get_experiment_dir(\n",
        "    experiment_name=\"train-from-pid-data\", \n",
        "    experiment_date=\"2026-02-16\", \n",
        "    experiment_time=\"16-58-13\"\n",
        ")\n",
        "\n",
        "CONFIG_PATH = EXPERIMENT_DIR_PATH / \"config.yaml\"\n",
        "config = load_config(CONFIG_PATH)\n",
        "print(f\"Эксперимент: {config.experiment_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Анализ процесса обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def parse_train_logs(file_path):\n",
        "    rows = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            entry = json.loads(line)\n",
        "            if entry.get(\"event\") != \"step\":\n",
        "                continue\n",
        "            rows.append({\n",
        "                'step': entry['step'],\n",
        "                'loss_q1': entry.get('loss_q1', np.nan),\n",
        "                'loss_q2': entry.get('loss_q2', np.nan),\n",
        "                'actor_loss': entry.get('actor_loss', np.nan),\n",
        "                'buffer_size': entry.get('buffer_size', np.nan),\n",
        "                'alpha': entry.get('alpha', np.nan),\n",
        "                'alpha_loss': entry.get('alpha_loss', np.nan),\n",
        "                'time': entry.get('time', np.nan),\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TRAIN_LOG_PATH = EXPERIMENT_DIR_PATH / config.training.log_file\n",
        "loss_df = parse_train_logs(TRAIN_LOG_PATH)\n",
        "print(f\"Загружено {len(loss_df)} записей из логов обучения\")\n",
        "print(f\"Диапазон шагов обучения: {loss_df['step'].min()} - {loss_df['step'].max()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
        "\n",
        "axes[0].plot(loss_df['step'], loss_df['loss_q1'], 'b-', alpha=0.7, label='Q1 Loss')\n",
        "axes[0].set_title('Q1 Loss')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(loss_df['step'], loss_df['loss_q2'], 'g-', alpha=0.7, label='Q2 Loss')\n",
        "axes[1].set_title('Q2 Loss')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(loss_df['step'], loss_df['loss_q1'] + loss_df['loss_q2'], 'r--', alpha=0.7, label='Sum (Q1 + Q2)')\n",
        "axes[2].set_title('Sum (Q1 + Q2)')\n",
        "axes[2].set_xlabel('Step')\n",
        "axes[2].set_ylabel('Loss')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "actor_loss_df = loss_df[loss_df['actor_loss'].notna()]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.plot(actor_loss_df['step'], actor_loss_df['actor_loss'], 'r-', alpha=0.7)\n",
        "plt.title('Actor Loss')\n",
        "\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(loss_df['step'], loss_df['buffer_size'], 'm-', alpha=0.7)\n",
        "plt.title('Buffer Size')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Size')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Анализ работы обученной модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "\n",
        "from nn_laser_stabilizer.rl.data.replay_buffer import ReplayBuffer\n",
        "from nn_laser_stabilizer.rl.model.actor import load_actor_from_path\n",
        "from nn_laser_stabilizer.rl.networks.factory import NetworkType\n",
        "\n",
        "ACTOR_PATH = EXPERIMENT_DIR_PATH / \"models\" / \"actor.pth\"  \n",
        "BUFFER_PATH = EXPERIMENT_DIR_PATH / \"data\" / \"replay_buffer.pth\"\n",
        "\n",
        "buffer = ReplayBuffer.load(BUFFER_PATH)\n",
        "print(f\"ReplayBuffer loaded. Size: {len(buffer)} / capacity={buffer.capacity}\")\n",
        "\n",
        "actor_path = ACTOR_PATH.resolve()\n",
        "if not actor_path.exists():\n",
        "    raise FileNotFoundError(f\"Actor model not found: {actor_path}\")\n",
        "\n",
        "print(f\"\\nLoading actor from: {actor_path}\")\n",
        "\n",
        "network_type_str = config.algorithm.actor.network.type\n",
        "network_type = NetworkType(network_type_str)\n",
        "\n",
        "actor = load_actor_from_path(actor_path, network_type)\n",
        "actor.eval()\n",
        "\n",
        "print(f\"Actor loaded successfully (type: {network_type_str})\")\n",
        "\n",
        "buffer_size = len(buffer)\n",
        "observations = buffer.observations[:buffer_size]\n",
        "true_actions = buffer.actions[:buffer_size]\n",
        "rewards = buffer.rewards[:buffer_size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Analyzing {buffer_size} transitions...\")\n",
        "\n",
        "predicted_actions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    if network_type == NetworkType.LSTM:\n",
        "        hidden_state = None\n",
        "        for i in range(buffer_size):\n",
        "            obs = observations[i]\n",
        "            action, options = actor.act(obs, {'hidden_state': hidden_state})\n",
        "            hidden_state = options.get('hidden_state')\n",
        "            predicted_actions.append(action)\n",
        "        \n",
        "        predicted_actions = torch.stack(predicted_actions, dim=0)\n",
        "    else:\n",
        "        batch_size = 1024\n",
        "        for i in range(0, buffer_size, batch_size):\n",
        "            end_idx = min(i + batch_size, buffer_size)\n",
        "            batch_obs = observations[i:end_idx]\n",
        "            batch_actions, _ = actor.act(batch_obs)\n",
        "            predicted_actions.append(batch_actions)\n",
        "        \n",
        "        predicted_actions = torch.cat(predicted_actions, dim=0)\n",
        "\n",
        "print(f\"Predictions completed. Shape: {predicted_actions.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mse = torch.mean((predicted_actions - true_actions) ** 2).item()\n",
        "mae = torch.mean(torch.abs(predicted_actions - true_actions)).item()\n",
        "\n",
        "action_dim = true_actions.shape[1]\n",
        "mse_per_dim = []\n",
        "mae_per_dim = []\n",
        "for dim in range(action_dim):\n",
        "    mse_dim = torch.mean((predicted_actions[:, dim] - true_actions[:, dim]) ** 2).item()\n",
        "    mae_dim = torch.mean(torch.abs(predicted_actions[:, dim] - true_actions[:, dim])).item()\n",
        "    mse_per_dim.append(mse_dim)\n",
        "    mae_per_dim.append(mae_dim)\n",
        "\n",
        "mean_reward = torch.mean(rewards).item()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Analysis Results:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total transitions analyzed: {buffer_size}\")\n",
        "print(f\"Mean reward: {mean_reward:.6f}\")\n",
        "print(\"\")\n",
        "print(\"Action prediction metrics:\")\n",
        "print(f\"  MSE (Mean Squared Error): {mse:.6f}\")\n",
        "print(f\"  MAE (Mean Absolute Error): {mae:.6f}\")\n",
        "\n",
        "if action_dim > 1:\n",
        "    print(\"  Per-dimension metrics:\")\n",
        "    for dim in range(action_dim):\n",
        "        print(f\"    Dim {dim}: MSE={mse_per_dim[dim]:.6f}, MAE={mae_per_dim[dim]:.6f}\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "obs_error = observations[:, 0].numpy()\n",
        "obs_control_output = observations[:, 3].numpy()\n",
        "true_actions_np = true_actions[:, 0].numpy()\n",
        "predicted_actions_np = predicted_actions[:, 0].numpy()\n",
        "rewards_np = rewards[:, 0].numpy()\n",
        "action_errors = predicted_actions_np - true_actions_np\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "axes[0, 0].scatter(true_actions_np, predicted_actions_np, alpha=0.5, s=1)\n",
        "axes[0, 0].plot([true_actions_np.min(), true_actions_np.max()], \n",
        "                [true_actions_np.min(), true_actions_np.max()], \n",
        "                'r--', lw=2, label='Perfect prediction')\n",
        "axes[0, 0].set_xlabel('True Action')\n",
        "axes[0, 0].set_ylabel('Predicted Action')\n",
        "axes[0, 0].set_title('True vs Predicted Actions')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].plot(action_errors, alpha=0.7, linewidth=0.5)\n",
        "axes[0, 1].axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
        "axes[0, 1].set_xlabel('Transition Index')\n",
        "axes[0, 1].set_ylabel('Action Error (Predicted - True)')\n",
        "axes[0, 1].set_title('Action Prediction Error Over Time')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 0].hist(action_errors, bins=50, alpha=0.7, edgecolor='black')\n",
        "axes[1, 0].axvline(x=0, color='r', linestyle='--', linewidth=1)\n",
        "axes[1, 0].set_xlabel('Action Error')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title('Distribution of Action Errors')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].plot(rewards_np, alpha=0.7, linewidth=0.5)\n",
        "axes[1, 1].axhline(y=mean_reward, color='r', linestyle='--', linewidth=2, \n",
        "                   label=f'Mean reward: {mean_reward:.4f}')\n",
        "axes[1, 1].set_xlabel('Transition Index')\n",
        "axes[1, 1].set_ylabel('Reward')\n",
        "axes[1, 1].set_title('Reward Over Time')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "axes[0].scatter(obs_error, action_errors, alpha=0.5, s=1)\n",
        "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
        "axes[0].set_xlabel('Observation Error (setpoint - process_variable)')\n",
        "axes[0].set_ylabel('Action Prediction Error')\n",
        "axes[0].set_title('Action Error vs Observation Error')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].scatter(obs_control_output, action_errors, alpha=0.5, s=1)\n",
        "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
        "axes[1].set_xlabel('Control Output')\n",
        "axes[1].set_ylabel('Action Prediction Error')\n",
        "axes[1].set_title('Action Error vs Control Output')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 6))\n",
        "\n",
        "sample_size = len(true_actions_np) \n",
        "indices = np.arange(sample_size)\n",
        "\n",
        "ax.plot(indices, true_actions_np[:sample_size], \n",
        "        label='True Action', \n",
        "        alpha=0.7, \n",
        "        linewidth=0.8,\n",
        "        color='blue')\n",
        "\n",
        "ax.plot(indices, predicted_actions_np[:sample_size], \n",
        "        label='Predicted Action (Model)', \n",
        "        alpha=0.7, \n",
        "        linewidth=0.8,\n",
        "        color='red')\n",
        "\n",
        "ax.set_xlabel('Transition Index (Time)', fontsize=12)\n",
        "ax.set_ylabel('Action Value', fontsize=12)\n",
        "ax.set_title('True Action vs Predicted Action Over Time', fontsize=14)\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import seaborn as sns\n",
        "\n",
        "observations = buffer.observations[:buffer_size]   # shape: [N, obs_dim]\n",
        "actions      = buffer.actions[:buffer_size]        # shape: [N, act_dim]\n",
        "rewards      = buffer.rewards[:buffer_size]        # shape: [N, 1]\n",
        "\n",
        "# observation = [error, error_prev, error_prev_prev, control_output]\n",
        "obs_error           = observations[:, 0].numpy()\n",
        "obs_error_prev      = observations[:, 1].numpy()\n",
        "obs_error_prev_prev = observations[:, 2].numpy()\n",
        "obs_control_output  = observations[:, 3].numpy()\n",
        "actions_np          = actions[:, 0].numpy()\n",
        "rewards_np          = rewards[:, 0].numpy()\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"error\": obs_error,\n",
        "    \"error_prev\": obs_error_prev,\n",
        "    \"error_prev_prev\": obs_error_prev_prev,\n",
        "    \"control_output\": obs_control_output,\n",
        "    \"action\": actions_np,\n",
        "    \"reward\": rewards_np,\n",
        "})\n",
        "print(\"\\nDataFrame head:\")\n",
        "display(df.head())\n",
        "print(\"\\nDataFrame describe:\")\n",
        "display(df.describe())\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
        "\n",
        "axes[0, 0].hist(df[\"error\"], bins=100, alpha=0.7, edgecolor=\"black\")\n",
        "axes[0, 0].set_title(\"Error distribution\")\n",
        "axes[0, 0].set_xlabel(\"error\")\n",
        "axes[0, 0].set_ylabel(\"count\")\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].hist(df[\"error_prev\"], bins=100, alpha=0.7, edgecolor=\"black\", color=\"tab:cyan\")\n",
        "axes[0, 1].set_title(\"Error prev distribution\")\n",
        "axes[0, 1].set_xlabel(\"error_prev\")\n",
        "axes[0, 1].set_ylabel(\"count\")\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 0].hist(df[\"control_output\"], bins=100, alpha=0.7, edgecolor=\"black\", color=\"orange\")\n",
        "axes[1, 0].set_title(\"Control output distribution\")\n",
        "axes[1, 0].set_xlabel(\"control_output\")\n",
        "axes[1, 0].set_ylabel(\"count\")\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].hist(df[\"action\"], bins=100, alpha=0.7, edgecolor=\"black\", color=\"green\")\n",
        "axes[1, 1].set_title(\"Action distribution\")\n",
        "axes[1, 1].set_xlabel(\"action\")\n",
        "axes[1, 1].set_ylabel(\"count\")\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2, 0].hist(df[\"reward\"], bins=100, alpha=0.7, edgecolor=\"black\", color=\"purple\")\n",
        "axes[2, 0].set_title(\"Reward distribution\")\n",
        "axes[2, 0].set_xlabel(\"reward\")\n",
        "axes[2, 0].set_ylabel(\"count\")\n",
        "axes[2, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2, 1].hist(df[\"error_prev_prev\"], bins=100, alpha=0.7, edgecolor=\"black\", color=\"tab:brown\")\n",
        "axes[2, 1].set_title(\"Error prev prev distribution\")\n",
        "axes[2, 1].set_xlabel(\"error_prev_prev\")\n",
        "axes[2, 1].set_ylabel(\"count\")\n",
        "axes[2, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "axes[0].scatter(df[\"error\"], df[\"action\"], s=1, alpha=0.3)\n",
        "axes[0].set_xlabel(\"error\")\n",
        "axes[0].set_ylabel(\"action\")\n",
        "axes[0].set_title(\"Action vs Error\")\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].scatter(df[\"error\"], df[\"reward\"], s=1, alpha=0.3, color=\"red\")\n",
        "axes[1].set_xlabel(\"error\")\n",
        "axes[1].set_ylabel(\"reward\")\n",
        "axes[1].set_title(\"Reward vs Error\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].scatter(df[\"control_output\"], df[\"action\"], s=1, alpha=0.3, color=\"green\")\n",
        "axes[2].set_xlabel(\"control_output\")\n",
        "axes[2].set_ylabel(\"action\")\n",
        "axes[2].set_title(\"Action vs Control Output\")\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "max_points = buffer_size\n",
        "idx = np.arange(max_points)\n",
        "\n",
        "fig, axes = plt.subplots(5, 1, figsize=(14, 14), sharex=True)\n",
        "\n",
        "axes[0].plot(idx, df[\"error\"].values[:max_points], linewidth=0.5)\n",
        "axes[0].set_ylabel(\"error\")\n",
        "axes[0].set_title(\"Error over time\")\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(idx, df[\"error_prev\"].values[:max_points], linewidth=0.5, color=\"tab:cyan\")\n",
        "axes[1].set_ylabel(\"error_prev\")\n",
        "axes[1].set_title(\"Error prev over time\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(idx, df[\"control_output\"].values[:max_points], linewidth=0.5, color=\"orange\")\n",
        "axes[2].set_ylabel(\"control_output\")\n",
        "axes[2].set_title(\"Control output over time\")\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "axes[3].plot(idx, df[\"action\"].values[:max_points], linewidth=0.5, color=\"green\")\n",
        "axes[3].set_ylabel(\"action\")\n",
        "axes[3].set_title(\"Action over time\")\n",
        "axes[3].grid(True, alpha=0.3)\n",
        "\n",
        "axes[4].plot(idx, df[\"reward\"].values[:max_points], linewidth=0.5, color=\"purple\")\n",
        "axes[4].set_ylabel(\"reward\")\n",
        "axes[4].set_xlabel(\"transition index\")\n",
        "axes[4].set_title(\"Reward over time\")\n",
        "axes[4].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "corr = df.corr(numeric_only=True)\n",
        "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "plt.title(\"Correlation matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nn-laser-stabilizer",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}